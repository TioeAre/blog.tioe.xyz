[{"authors":[],"categories":[{"title":"OpenCV","url":"/categories/opencv/"},{"title":"Probability","url":"/categories/probability/"}],"content":"","date":"September 18, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/study/ransac/","series":[],"smallImg":"","tags":[{"title":"OpenCV","url":"/tags/opencv/"},{"title":"Probability","url":"/tags/probability/"}],"timestamp":1695016665,"title":"RANSAC"},{"authors":[],"categories":[],"content":"本文受NICE-SLAM启发, 最初是想要设计一个网络, 使其能够将不同传感器的数据融合起来得到一个准确的无人机位置. 首先设计并训练一个用来融合数据的网络, 训练出的网络作为拟合位置的函数, 然后用神经隐式表达的方法, 给出一个无人机位置的先验, 然后用刚才预训练好的网络来decode出不同传感器可能的读数, 再将该读数与真实读到的数据做loss, 反向梯度计算出在什么样的位置下最有可能用这几种传感器读到这些读数. 从而得到最大似然的无人机位置.\n然后首先便需要训练一个能够起到decode作用的网络. 在训练之前我想要先训练一个正向融合数据的网络作为测试, 然后想到了FCN全卷积网络, 当时觉得全卷积网络不限置输入和输出的大小, 训练的是卷积核, 似乎刚好符合我想要训练一个融合不同传感器数值的函数的目的.\n为了训练网络, 首先需要数据. 在网上找到了t265的xacro的模型, 根据该模型将其转换为gazebo的px4无人机仿真中使用的sdf模型, 具体文件如下\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;sdf version=\u0026#39;1.5\u0026#39;\u0026gt; \u0026lt;model name=\u0026#39;realsense_t265\u0026#39;\u0026gt; \u0026lt;link name=\u0026#39;base_link\u0026#39;\u0026gt; \u0026lt;inertial\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 0 0\u0026lt;/pose\u0026gt; \u0026lt;mass\u0026gt;0.055\u0026lt;/mass\u0026gt; \u0026lt;inertia\u0026gt; \u0026lt;ixx\u0026gt;9.108e-05\u0026lt;/ixx\u0026gt; \u0026lt;ixy\u0026gt;0\u0026lt;/ixy\u0026gt; \u0026lt;ixz\u0026gt;0\u0026lt;/ixz\u0026gt; \u0026lt;iyy\u0026gt;2.51e-06\u0026lt;/iyy\u0026gt; \u0026lt;iyz\u0026gt;0\u0026lt;/iyz\u0026gt; \u0026lt;izz\u0026gt;8.931e-05\u0026lt;/izz\u0026gt; \u0026lt;/inertia\u0026gt; \u0026lt;/inertial\u0026gt; \u0026lt;collision name=\u0026#39;base_link_fixed_joint_lump__t265_pose_frame_collision\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;geometry\u0026gt; \u0026lt;box\u0026gt; \u0026lt;size\u0026gt;0.013 0.108 0.024\u0026lt;/size\u0026gt; \u0026lt;/box\u0026gt; \u0026lt;/geometry\u0026gt; \u0026lt;/collision\u0026gt; \u0026lt;visual name=\u0026#39;base_link_fixed_joint_lump__t265_pose_frame_visual\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 1.57 -0 1.57\u0026lt;/pose\u0026gt; \u0026lt;geometry\u0026gt; \u0026lt;mesh\u0026gt; \u0026lt;scale\u0026gt;0.001 0.001 0.001\u0026lt;/scale\u0026gt; \u0026lt;uri\u0026gt;model://realsense_t265/meshes/realsense_t265.stl\u0026lt;/uri\u0026gt; \u0026lt;/mesh\u0026gt; \u0026lt;/geometry\u0026gt; \u0026lt;/visual\u0026gt; \u0026lt;sensor name=\u0026#39;camera1\u0026#39; type=\u0026#39;wideanglecamera\u0026#39;\u0026gt; \u0026lt;camera\u0026gt; \u0026lt;horizontal_fov\u0026gt;6.283\u0026lt;/horizontal_fov\u0026gt; \u0026lt;image\u0026gt; \u0026lt;width\u0026gt;800\u0026lt;/width\u0026gt; \u0026lt;height\u0026gt;848\u0026lt;/height\u0026gt; \u0026lt;/image\u0026gt; \u0026lt;clip\u0026gt; \u0026lt;near\u0026gt;0.1\u0026lt;/near\u0026gt; \u0026lt;far\u0026gt;100\u0026lt;/far\u0026gt; \u0026lt;/clip\u0026gt; \u0026lt;lens\u0026gt; \u0026lt;type\u0026gt;custom\u0026lt;/type\u0026gt; \u0026lt;custom_function\u0026gt; \u0026lt;c1\u0026gt;1.05\u0026lt;/c1\u0026gt; \u0026lt;c2\u0026gt;4\u0026lt;/c2\u0026gt; \u0026lt;f\u0026gt;1\u0026lt;/f\u0026gt; \u0026lt;fun\u0026gt;tan\u0026lt;/fun\u0026gt; \u0026lt;/custom_function\u0026gt; \u0026lt;scale_to_hfov\u0026gt;1\u0026lt;/scale_to_hfov\u0026gt; \u0026lt;cutoff_angle\u0026gt;3.1415\u0026lt;/cutoff_angle\u0026gt; \u0026lt;env_texture_size\u0026gt;512\u0026lt;/env_texture_size\u0026gt; \u0026lt;/lens\u0026gt; \u0026lt;always_on\u0026gt;1\u0026lt;/always_on\u0026gt; \u0026lt;update_rate\u0026gt;30\u0026lt;/update_rate\u0026gt; \u0026lt;/camera\u0026gt; \u0026lt;plugin name=\u0026#39;camera_controller1\u0026#39; filename=\u0026#39;libgazebo_ros_camera.so\u0026#39;\u0026gt; \u0026lt;alwaysOn\u0026gt;1\u0026lt;/alwaysOn\u0026gt; \u0026lt;updateRate\u0026gt;30\u0026lt;/updateRate\u0026gt; \u0026lt;cameraName\u0026gt;t265/fisheye1\u0026lt;/cameraName\u0026gt; \u0026lt;imageTopicName\u0026gt;image_raw\u0026lt;/imageTopicName\u0026gt; \u0026lt;cameraInfoTopicName\u0026gt;camera_info\u0026lt;/cameraInfoTopicName\u0026gt; \u0026lt;frameName\u0026gt;\u0026#34;t265_fisheye1_optical_frame\u0026#34;\u0026lt;/frameName\u0026gt; \u0026lt;hackBaseline\u0026gt;0.07\u0026lt;/hackBaseline\u0026gt; \u0026lt;distortionK1\u0026gt;-0.007419134024530649\u0026lt;/distortionK1\u0026gt; \u0026lt;distortionK2\u0026gt;0.041209351271390915\u0026lt;/distortionK2\u0026gt; \u0026lt;distortionK3\u0026gt;-0.03811917081475258\u0026lt;/distortionK3\u0026gt; \u0026lt;distortionT1\u0026gt;0.006366158835589886\u0026lt;/distortionT1\u0026gt; \u0026lt;distortionT2\u0026gt;0.0\u0026lt;/distortionT2\u0026gt; \u0026lt;CxPrime\u0026gt;416.00531005859375\u0026lt;/CxPrime\u0026gt; \u0026lt;Cx\u0026gt;16.00531005859375\u0026lt;/Cx\u0026gt; \u0026lt;Cy\u0026gt;403.38909912109375\u0026lt;/Cy\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;pose\u0026gt;0.01 0.042 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/sensor\u0026gt; \u0026lt;sensor name=\u0026#39;camera2\u0026#39; type=\u0026#39;wideanglecamera\u0026#39;\u0026gt; \u0026lt;camera\u0026gt; \u0026lt;horizontal_fov\u0026gt;6.283\u0026lt;/horizontal_fov\u0026gt; \u0026lt;image\u0026gt; \u0026lt;width\u0026gt;800\u0026lt;/width\u0026gt; \u0026lt;height\u0026gt;848\u0026lt;/height\u0026gt; \u0026lt;/image\u0026gt; \u0026lt;clip\u0026gt; \u0026lt;near\u0026gt;0.1\u0026lt;/near\u0026gt; \u0026lt;far\u0026gt;100\u0026lt;/far\u0026gt; \u0026lt;/clip\u0026gt; \u0026lt;lens\u0026gt; \u0026lt;type\u0026gt;custom\u0026lt;/type\u0026gt; \u0026lt;custom_function\u0026gt; \u0026lt;c1\u0026gt;1.05\u0026lt;/c1\u0026gt; \u0026lt;c2\u0026gt;4\u0026lt;/c2\u0026gt; \u0026lt;f\u0026gt;1\u0026lt;/f\u0026gt; \u0026lt;fun\u0026gt;tan\u0026lt;/fun\u0026gt; \u0026lt;/custom_function\u0026gt; \u0026lt;scale_to_hfov\u0026gt;1\u0026lt;/scale_to_hfov\u0026gt; \u0026lt;cutoff_angle\u0026gt;3.1415\u0026lt;/cutoff_angle\u0026gt; \u0026lt;env_texture_size\u0026gt;512\u0026lt;/env_texture_size\u0026gt; \u0026lt;/lens\u0026gt; \u0026lt;always_on\u0026gt;1\u0026lt;/always_on\u0026gt; \u0026lt;update_rate\u0026gt;30\u0026lt;/update_rate\u0026gt; \u0026lt;/camera\u0026gt; \u0026lt;plugin name=\u0026#39;camera_controller2\u0026#39; filename=\u0026#39;libgazebo_ros_camera.so\u0026#39;\u0026gt; \u0026lt;alwaysOn\u0026gt;1\u0026lt;/alwaysOn\u0026gt; \u0026lt;updateRate\u0026gt;30\u0026lt;/updateRate\u0026gt; \u0026lt;cameraName\u0026gt;t265/fisheye2\u0026lt;/cameraName\u0026gt; \u0026lt;imageTopicName\u0026gt;image_raw\u0026lt;/imageTopicName\u0026gt; \u0026lt;cameraInfoTopicName\u0026gt;camera_info\u0026lt;/cameraInfoTopicName\u0026gt; \u0026lt;frameName\u0026gt;t265_fisheye2_optical_frame\u0026lt;/frameName\u0026gt; \u0026lt;hackBaseline\u0026gt;0.07\u0026lt;/hackBaseline\u0026gt; \u0026lt;distortionK1\u0026gt;-0.007419134024530649\u0026lt;/distortionK1\u0026gt; \u0026lt;distortionK2\u0026gt;0.041209351271390915\u0026lt;/distortionK2\u0026gt; \u0026lt;distortionK3\u0026gt;-0.03811917081475258\u0026lt;/distortionK3\u0026gt; \u0026lt;distortionT1\u0026gt;0.006366158835589886\u0026lt;/distortionT1\u0026gt; \u0026lt;distortionT2\u0026gt;0.0\u0026lt;/distortionT2\u0026gt; \u0026lt;CxPrime\u0026gt;416.00531005859375\u0026lt;/CxPrime\u0026gt; \u0026lt;Cx\u0026gt;16.00531005859375\u0026lt;/Cx\u0026gt; \u0026lt;Cy\u0026gt;403.38909912109375\u0026lt;/Cy\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;pose\u0026gt;0.01 -0.022 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/sensor\u0026gt; \u0026lt;gravity\u0026gt;1\u0026lt;/gravity\u0026gt; \u0026lt;sensor name=\u0026#39;t265_imu\u0026#39; type=\u0026#39;imu\u0026#39;\u0026gt; \u0026lt;always_on\u0026gt;1\u0026lt;/always_on\u0026gt; \u0026lt;update_rate\u0026gt;500\u0026lt;/update_rate\u0026gt; \u0026lt;visualize\u0026gt;0\u0026lt;/visualize\u0026gt; \u0026lt;plugin name=\u0026#39;imu_plugin\u0026#39; filename=\u0026#39;libgazebo_ros_imu_sensor.so\u0026#39;\u0026gt; \u0026lt;topicName\u0026gt;t265/gyro/sample\u0026lt;/topicName\u0026gt; \u0026lt;bodyName\u0026gt;t265_pose_frame\u0026lt;/bodyName\u0026gt; \u0026lt;updateRateHZ\u0026gt;500.0\u0026lt;/updateRateHZ\u0026gt; \u0026lt;gaussianNoise\u0026gt;0.000001\u0026lt;/gaussianNoise\u0026gt; \u0026lt;xyzOffset\u0026gt;0 0 0\u0026lt;/xyzOffset\u0026gt; \u0026lt;rpyOffset\u0026gt;0 0 0\u0026lt;/rpyOffset\u0026gt; \u0026lt;frameName\u0026gt;t265_link\u0026lt;/frameName\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/sensor\u0026gt; \u0026lt;plugin name=\u0026#39;p3d_base_controller\u0026#39; filename=\u0026#39;libgazebo_ros_p3d.so\u0026#39;\u0026gt; \u0026lt;alwaysOn\u0026gt;1\u0026lt;/alwaysOn\u0026gt; \u0026lt;updateRate\u0026gt;500\u0026lt;/updateRate\u0026gt; \u0026lt;topicName\u0026gt;t265/odom/sample\u0026lt;/topicName\u0026gt; \u0026lt;gaussianNoise\u0026gt;0.001\u0026lt;/gaussianNoise\u0026gt; \u0026lt;frameName\u0026gt;world\u0026lt;/frameName\u0026gt; \u0026lt;xyzOffsets\u0026gt;0 0 0\u0026lt;/xyzOffsets\u0026gt; \u0026lt;rpyOffsets\u0026gt;0 0 0\u0026lt;/rpyOffsets\u0026gt; \u0026lt;bodyName\u0026gt;base_link\u0026lt;/bodyName\u0026gt; \u0026lt;xyzOffset\u0026gt;0 0 0\u0026lt;/xyzOffset\u0026gt; \u0026lt;rpyOffset\u0026gt;0 -0 0\u0026lt;/rpyOffset\u0026gt; \u0026lt;ignition::corrected_offsets\u0026gt;1\u0026lt;/ignition::corrected_offsets\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;frame name=\u0026#39;t265_accel_joint\u0026#39; attached_to=\u0026#39;t265_link\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_accel_frame\u0026#39; attached_to=\u0026#39;t265_accel_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye1_optical_joint\u0026#39; attached_to=\u0026#39;t265_fisheye1_frame\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0.01 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye1_optical_frame\u0026#39; attached_to=\u0026#39;t265_fisheye1_optical_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye1_joint\u0026#39; attached_to=\u0026#39;t265_link\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0.042 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye1_frame\u0026#39; attached_to=\u0026#39;t265_fisheye1_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye2_optical_joint\u0026#39; attached_to=\u0026#39;t265_fisheye2_frame\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0.01 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye2_optical_frame\u0026#39; attached_to=\u0026#39;t265_fisheye2_optical_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye2_joint\u0026#39; attached_to=\u0026#39;t265_link\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 -0.022 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_fisheye2_frame\u0026#39; attached_to=\u0026#39;t265_fisheye2_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_gyro_joint\u0026#39; attached_to=\u0026#39;t265_link\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_gyro_frame\u0026#39; attached_to=\u0026#39;t265_gyro_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_joint\u0026#39; attached_to=\u0026#39;t265_pose_frame\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_link\u0026#39; attached_to=\u0026#39;t265_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_pose_frame_joint\u0026#39; attached_to=\u0026#39;t265_odom_frame\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_pose_frame\u0026#39; attached_to=\u0026#39;t265_pose_frame_joint\u0026#39;/\u0026gt; \u0026lt;frame name=\u0026#39;t265_odom_frame_joint\u0026#39; attached_to=\u0026#39;base_link\u0026#39;\u0026gt; \u0026lt;pose\u0026gt;0 0 0 0 -0 0\u0026lt;/pose\u0026gt; \u0026lt;/frame\u0026gt; \u0026lt;frame name=\u0026#39;t265_odom_frame\u0026#39; attached_to=\u0026#39;t265_odom_frame_joint\u0026#39;/\u0026gt; \u0026lt;/link\u0026gt; \u0026lt;/model\u0026gt; \u0026lt;/sdf\u0026gt; \u0026lt;!-- vim: set et ft=xml fenc=utf-8 ff=unix sts=0 sw=2 ts=2 : --\u0026gt; 在gazebo中构建一个包含由imu, t265, px4flow, gps这几中传感器的四旋翼无人机模型, 然后使用QGroundControl地面站连接飞机进入任务模式, 按照预定的模拟线路在仿真中飞行1小时左右, 期间写了一个python脚本用于ros采集数据并录制了一个rosbag. 为了提高网络的鲁棒性, 在收集数据的过程中随机5%的数据进行清零或增加0-50内的随机数作为该传感器失灵的情况\n最终收集到的数据以n*100*4*15的形式储存在json文件中, n指n个batch, 100指这1秒100hz的连续数据, 4指上述四个传感器, 15分别为三维xyz位置, 三维传感器相对于飞机中心点的偏移量, 三维数据是否可用, 三维数据是否发生跳变和三维传感器误差种类. 其中前六维为float32, 后九维为bool\n经过多次修改测试后的网络架构如下图所示\n代码实现见https://github.com/TioeAre/data_fusion_network\n然而最终效果却不尽人意, 网络并没有学习到如何判断某个数据是否正常等条件, 反而最后训练出的网络起到了类似剔除不良数据的作用. 网络输出能够与真实数据保持一致性, 能够剔除掉之前手动加入的大误差或清零部分数据\n后续与曹老师的交流发现我的网络结构设计的还是太简单了, 只用位置, 也没有考虑到每个传感器的性质, 没有考虑到姿态.\n后续准备再继续阅读学习网络拟合imu的相关论文, 做好设计后再进行尝试\n","date":"June 17, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/project/nnsensorfusion/","series":[],"smallImg":"","tags":[],"timestamp":1687001445,"title":"使用神经网络融合位置数据(第一版)"},{"authors":[],"categories":[{"title":"实验记录","url":"/categories/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/"}],"content":"一、实验目的\n使用光流，无人机定位模式 找无人机突然失控的原因 二、实验过程记录\n首先使用姿态模式起飞，无异常情况 换为位置模式起飞，起飞后存在左右小幅度摇摆偏移 降落未断电后重新起飞，无人机失控向左后方偏移然后遥控器急停 未断电重新尝试解锁，不能解锁并报错如下图 断电后可以解锁，位置模式仍然失控\n重新使用姿态起飞，姿态仍然正常\n三、实验结果\n查看日志发现姿态估计结果异常，飞机水平放置时，飞控仍会以为飞机存在水平倾斜，判断是姿态估计出错导致的失控 还发现报错imu stopped aidding，看源码发现在ekf滤波开始前有相关判断，并在下图两处为PV_AidingMode赋值为AID_NONE然后报错，推测报错原因为参数里ek3_src1_yaw没有赋值 继续跳转发现AID_NONE会导致滤波结果重置和yaw方向的融合，推测是这个原因导致的姿态估计错误 然而ek3_src1_yaw这个参数只有none, Compass, GPS, GPS with Compass Fallback, ExternalNav, GSF几种，只能选compass，但是又会报错compass unhealthy，尚未解决 2023.06.24:\n后续发现kakute mini飞控中并没有磁力计, 板载imu只有加速度记和陀螺仪. 外置磁力计后能够跳出该判断条件, 避免估计的状态量和观测值清零. 在大角度旋转无人机, 状态因光流测算异常而估计错误后能够自动返回正确的姿态\n此时仍存在室内定位漂移以及使用光流的激光测高模块会使高度估计失控的情况, 下一步买回来自带陀螺仪补偿的px4flow后再进行实验找到问题\n","date":"June 15, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/daily/2023.06.15/","series":[],"smallImg":"","tags":[{"title":"实验记录","url":"/tags/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/"}],"timestamp":1686828645,"title":"调试室内定位"},{"authors":[],"categories":[{"title":"slam","url":"/categories/slam/"}],"content":" ​\tNICE-SLAM是一种利用神经隐式表达的稠密slam系统,主要维护了一个4层的feature grids的全局地图, 每个grid表示当前相机位姿下的局部的场景, 4个层级分别表示由粗到细的三个深度图和一个rgb层. 每个层级的每个grid储存32维的特征向量.\n​\t从右到左: 系统相当于一个场景渲染器。接收场景的特征网格和相机位姿，生成带有深度信息和色彩信息的RGB-D图像（深度信息处理为场景三维模型，色彩信息处理为贴图）；\n​\t从左往右: 输入采集的RGB-D图像流（ground truth），把右边渲染出的RGB-D估计值与坐边输入的RGB-D实际值比对，分别计算关于深度和色彩的损失函数，并且将损失函数沿着可微分渲染器反向传播。经过迭代，对特征网格和相机位姿进行优化（将损失函数降到最低），完成建图（Mapping）和定位（Tracking）\n​\t在建图过程中, NICE-SLAM使用如上图所示的架构, 首先得到一个先验的相机位姿, 然后在这个相机位姿下的不同层级的grid中按照相机朝向的ray进行随机采样一些点, 再用线性插值的方法得到每个grid中点的32维feature. 再用一个预训练好的decoder对这些先验的 feature点进行decode成深度图和彩色图. 再将decode出的深度图和彩色图与实际深度相机得到的深度图和彩色图进行对比做loss. 然后反过来去训练输入的feature上的点. 最终的到能够decode成正确深度图和彩色图的feature grid. 此时的feature grid就是slam建立起的稠密地图\n​\tNICE-SLAM建图的过程是tracking和mapping交替进行。Mapping的过程是以当前相机的RGB和depth作为真值，根据Feature grids渲染得到的RGB和depth作为预测值，构建几何或光度误差损失函数，同时对相机位姿和Feature grids中的feature进行优化。Tracking的过程就是基于Feature grids和估计的位姿渲染depth和rgb，通过和相机实际采集的depth和rgb对比，从而优化相机位姿。\n​\tNICE-SLAM的优势主要是与iMAP相比, iMAP是基于经典的NERF架构, 整个网络使用一个大的MLP来储存场景信息, 难以对大型场景进行细节建图, 并且在每次更新地图时都要对整个MLP的网络进行更新, 容易将已经学习到的细节覆盖掉造成场景的遗忘. 相比而言, NICE-SLAM使用 由粗到细的分场景表达, 对三个层级分别应用不同的预训练好的MLP. 并且能够允许对每个grid进行局部更新. 此时分辨率较低的coarse主要进行大场景的渲染, 因为分辨率较高, 所以能够允许网络对相机观测不到的场景也进行一个简单的预判, 大致填充没有观测到的区域. 分辨率最高的fine层级则负责对局部场景进行建模, 保证了网络渲染出的地图的精确度. 但是NICE-SLAM中对rgb信息的训练只用了一个MLP, 因此在回复每个体素的rgb时会存在一定的遗忘问题.\n​\t在网络架构中最右侧的黄色部分表示分层的特征网络, 其中红色的点表示空间中的样本点P, 与P相连的深蓝色的点表示三线性插值得到的几何参数表示为$\\theta$. 此时这个样本点周围的函数场就表示为$\\phi_\\theta(p)$, 即深蓝色点与红色点的连线. 与每个特征网络对应的是各自的decode $f$, decode将样本点P和feature map中的值解码成体素的占用概率$o_p$, 针对分辨率最高的fine层级额外添加了一个独立的网络$\\psi_\\omega$用于表示rgb色彩信息对应decoder $g_\\omega$, 在分辨率最高的层级中渲染出彩色的稠密地图\n​\t分辨率最低的corase层级仅用于估计没有观测到的场景中的缝隙$o^0_p=f^0(p,\\phi^0_\\theta(p))$\n​\t在渲染过程中, mid层级的decoder将输入样本点座标和feature grid计算出基础占用$o^1_p=f^1(p,\\phi^1_\\theta(p))$\n​\tfine层级将高分辨率的样本点和grid以及上一步mid层级得到的输出一同作为输入, $\\Delta o^1_p=f^2(p,\\phi^1_\\theta(p),\\phi^2_\\theta(p))$, $o_p=o^1_p+\\Delta o^1_p$\n在色彩信息的渲染中, 使用$c_p=g_\\omega(p, \\psi _\\omega(p))$\n​\t得到渲染输出后, 对于每条ray，在corase和fine两个层面的深度级别的深度，以及颜色可以被呈现为\n$$ \\hat{D}^c=\\sum^{N}{i=1}\\omega^c_id_i,\\ \\hat{D}^f=\\sum^{N}{i=1}\\omega^f_id_i,\\ \\hat{I}=\\sum^{N}_{i=1}\\omega^f_ic_i $$\n沿每条ray 的深度值\n$$ \\hat{D}^c_{var}=\\sum^{N}{i=1}\\omega^c_i(\\hat{D}^c-d_i)^2,\\ \\hat{D}^f{var}=\\sum^{N}_{i=1}\\omega^f_i(\\hat{D}^f-d_i)^2 $$\n​\t建图从当前帧和选定的关键帧中均匀采样总共M个像素. 然后采用分阶段的方式来最小化几何和光度损失. 几何损失只是corase或fine层次上观测值和预测深度之间的L1损失 $$ \\iota^l_g=\\frac{1}{M}\\sum_{m=1}^{M}\\begin{vmatrix} D_m-\\hat{D}^l_m \\end{vmatrix} $$\n​\t对于M个采样像素，光度损失也是渲染和观察颜色值之间的L1损失 $$ \\iota_p=\\frac{1}{M}\\sum_{m=1}^{M}\\begin{vmatrix}I_m-\\hat{I}^l_m\\end{vmatrix} $$\n​\t在第一阶段, 只优化中级特征网格$ϕ_θ^1$, 使用几何损失$L_g^f$, 然后共同优化中级和精细级$ϕ_θ^1$, $ϕ_θ^2$特征与相同的精细级深度损失$L_g^f$, 最后，进行局部BA, 共同优化所有层级的特征网格, 颜色解码器以及K个选定关键帧的相机外部参数$R_i, t_i$ $$ min_{\\theta,\\omega,{R_i,t_i}}(\\iota^c_g+\\iota^f_g+\\lambda_p\\iota_p) $$ ​\t相机跟踪中对当前$M_t$的像素进行采样, 应用光度损失\n$$ \\iota_{g_mvar}=\\frac{1}{M} \\sum^{M_t}{m=1} (\\frac{\\begin{vmatrix} D_m-\\hat{D}^c_m \\end{vmatrix}}{\\sqrt{\\hat{D}^c{var}}}+\\frac{\\begin{vmatrix} D_m-\\hat{D}^f_m \\end{vmatrix}}{\\sqrt{\\hat{D}^f_{var}}}) $$\n几何损失 $$ min_{R,t}(\\iota_{g_mvar+\\lambda_{pt}\\iota_p}) $$\n","date":"May 30, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/study/nice_slam/","series":[],"smallImg":"","tags":[{"title":"slam","url":"/tags/slam/"}],"timestamp":1685411865,"title":"Nice Slam论文解析"},{"authors":[],"categories":[{"title":"Multi-Sensor","url":"/categories/multi-sensor/"}],"content":" ekf3用于估计位姿, 导航坐标系为NED(北东地)\n数学推导参考PX4和APM导航解算过程推导-24维状态EKF\nmatlab见InertialNav-Github\nekf2代码介绍EKF2\nAP_NavEKF3.cpp bool InitialiseFilter() 用于初始化滤波器, 滤波器可以有好几个core, 每个core或lane对应一种传感器和imu的状态估计, 初始化时EK3_PRIMARY可以在为某个core或lane. 但之后可能会在运行时被修改成其他的lane. Affinity允许不设置首要lane, 并用lane error存储不同传感器的差值, 使系统能够效果最好的作为状态估计. lane error随时间积累并通过EK3_ERR_THRESH参数设置non-primary and primary lane比较的阈值, 当超过阈值时会切换用于估计的传感器(下文中的所有core与lane几乎是可以表示同一个东西)\n函数中首先获取循环一次的时间, 每个imu对应一个core, 一个imu不能用于多个滤波器, 然后为ekf core申请内存, 如果申请成功的话就调用NavEKF3_core()创建每个core, 然后调用resetCoreErrors()初始化上文提到的每个core对应的lane error\nNavEKF3_core()初始化时使用了libraries/AP_NavEKF3/AP_NavEKF3_core.cpp中几个函数InitialiseFilterBootstrap() InitialiseVariables()\u0026amp;\u0026amp;CovarianceInit() InitialiseVariablesMag()(依次调用), 使系统运行中能够在必要时初始化滤波器(在UpdateFilter()中, 如果filterStatus.value==0的话会调用InitialiseFilterBootstrap()重新初始化)\n协方差初始化时, 初始化为对角矩阵\nmemset(\u0026amp;P[0][0], 0, sizeof(P)); // define the initial angle uncertainty as variances for a rotation vector Vector3F rot_vec_var; rot_vec_var.x = rot_vec_var.y = rot_vec_var.z = sq(0.1f); // reset the quaternion state covariances CovariancePrediction(\u0026amp;rot_vec_var); // velocities P[4][4] = sq(frontend-\u0026gt;_gpsHorizVelNoise); P[5][5] = P[4][4]; P[6][6] = sq(frontend-\u0026gt;_gpsVertVelNoise); // positions P[7][7] = sq(frontend-\u0026gt;_gpsHorizPosNoise); P[8][8] = P[7][7]; P[9][9] = sq(frontend-\u0026gt;_baroAltNoise); // gyro delta angle biases P[10][10] = sq(radians(InitialGyroBiasUncertainty() * dtEkfAvg)); P[11][11] = P[10][10]; P[12][12] = P[10][10]; // delta velocity biases P[13][13] = sq(ACCEL_BIAS_LIM_SCALER * frontend-\u0026gt;_accBiasLim * dtEkfAvg); P[14][14] = P[13][13]; P[15][15] = P[13][13]; // earth magnetic field P[16][16] = sq(frontend-\u0026gt;_magNoise); P[17][17] = P[16][16]; P[18][18] = P[16][16]; // body magnetic field P[19][19] = sq(frontend-\u0026gt;_magNoise); P[20][20] = P[19][19]; P[21][21] = P[19][19]; // wind velocities P[22][22] = 0.0f; P[23][23] = P[22][22]; bool coreBetterScore() 这个函数用于判断是否切换主要的滤波器\nbool NavEKF3::coreBetterScore(uint8_t new_core, uint8_t current_core) const { const NavEKF3_core \u0026amp;oldCore = core[current_core]; const NavEKF3_core \u0026amp;newCore = core[new_core]; if (!newCore.healthy()) { // never consider a new core that isn\u0026#39;t healthy return false; } if (newCore.have_aligned_tilt() != oldCore.have_aligned_tilt()) { // tilt alignment is most critical, if one is tilt aligned and // the other isn\u0026#39;t then use the tilt aligned lane return newCore.have_aligned_tilt(); } if (newCore.have_aligned_yaw() != oldCore.have_aligned_yaw()) { // yaw alignment is next most critical, if one is yaw aligned // and the other isn\u0026#39;t then use the yaw aligned lane return newCore.have_aligned_yaw(); } // if both cores are aligned then look at relative error scores return coreRelativeErrors[new_core] \u0026lt; coreRelativeErrors[current_core]; } 先判断新的core是否正常工作, 然后判断这两个core是否有倾斜对齐, 返回有对齐的那一个, 然后判断是否对齐的yaw, 最后再判断两者lane error的大小\nvoid UpdateFilter() struct state_elements { QuaternionF quat; // quaternion defining rotation from local NED earth frame to body frame 0..3 Vector3F velocity; // velocity of IMU in local NED earth frame (m/sec) 4..6 Vector3F position; // position of IMU in local NED earth frame (m) 7..9 Vector3F gyro_bias; // body frame delta angle IMU bias vector (rad) 10..12 Vector3F accel_bias; // body frame delta velocity IMU bias vector (m/sec) 13..15 Vector3F earth_magfield; // earth frame magnetic field vector (Gauss) 16..18 Vector3F body_magfield; // body frame magnetic field vector (Gauss) 19..21 Vector2F wind_vel; // horizontal North East wind velocity vector in local NED earth frame (m/sec) 22..23 }; 先判断两次更新之间是否丢帧, 没有丢帧的话就对每个core UpdateFilter()\n然后判断当前的主要core是否正常工作, 如果不正常工作的话就切换core, 此判断只有在主要core正常运行10s后才开始使用\n当无人机能够解锁并且主core已经正常运行了超过10s之后进入判断是否要切换primary core(但runCoreSelection这个变量好像只在上面这一个地方被赋值, 也就是说只要此时的primary core正常工作10s之后, 每次UpdateFilter()运行时都会判断是否要切换core)\n然后开始计算lane error,\nlane error计算的是每个传感器的观测值velPosObs与ekf滤波后的状态变量stateStruct中的值相减之后做平方和(即innovation)再除以方差的和, 感觉本质上就是把这个传感器的数据跟对应的imu的数据做个比较, 如果二者相差大的话就说明这个core没有正常工作\n计算每个lane error之后再计算了其他的lane error与primary lane error之间的差值, 并将差值累加起来\n如果这个差值小于阈值的话并且距离上一次切换时间超过10s的话并且yaw已经对齐了的话, 就发生切换并重置累加起来的每个lane error\n更新滤波器的步骤主要也是调用了libraries/AP_NavEKF3/AP_NavEKF3_core.cpp中的UpdateFilter()函数\nNavEKF3_core::UpdateFilter()中先调用update_sensor_selection()更新每个core中传感器的选择, 具体步骤以update_gps_selection()为例\nconst auto \u0026amp;gps = dal.gps();\t// 先获取gps的数据对象, 储存当前的gps信息 selected_gps = gps.primary_sensor();\t// 设置当前的gps索引 preferred_gps = selected_gps; if (frontend-\u0026gt;_affinity \u0026amp; EKF_AFFINITY_GPS) {\t// 按位与判断gps是否符合affinity if (core_index \u0026lt; gps.num_sensors() ) { preferred_gps = core_index; }\t// 判断是否有超过core_index个gps, 也就是当前的core是否有对应的gps if (gps.status(preferred_gps) \u0026gt;= AP_DAL_GPS::GPS_OK_FIX_3D) {\t// 检查当前的第code_index个gps也就是当前core对应的第n个gps(如果有的话)的数据是否满足gps的3D定位的标准, 如果满足的话就改用当前core_index的gps selected_gps = preferred_gps; } } 然后调用controlFilterModes()检查飞行状态(电机状态, 飞行模式, 是否自动设置风力和磁场变化, 是否对齐飞行姿态, 设置导航辅助模式(导航辅助模式主要就是判断用了哪些传感器, 如果用了gps或气压计或视觉定位的话就是绝对测量, 否则用了光流或者机体坐标系原始数据的话设置为相对测量))\n然后调用readIMUData()读取imu的数据\ndtIMUavg = 0.02f * constrain_ftype(ins.get_loop_delta_t(),0.5f * dtIMUavg, 2.0f * dtIMUavg) + 0.98f * dtIMUavg;\t// 先计算imu频率时使用一个低通滤波 然后判断切换加速度计和陀螺仪, 如果切换的话直接赋值原先的偏差到切换来的上. 然后判断运动状态, 主要代码计算了陀螺仪和加速度计的的差值和差值的比例, 然后判断当前是否在地面上并且没有运动\nlearnInactiveBiases()中主要是对陀螺仪和加速度计的偏差进行修正和赋值, 如果是activate gyro或accel的话就直接将ekf估计出的bias复制给他, 如果是inactivate的话就计算出他与activate之间的误差然后乘一个很小的缩放系数慢慢的将他对齐到activate的\n之后读取累积的imu数据, 然后矫正传感器误差\n开始更新时首先调用UpdateStrapdownEquationsNED()对imu的数据进行累加和转换, 转换到导航坐标系, 然后计算了imu测量出的各轴速度和加速度\n之后调用CovariancePrediction()更新协方差的先验\n调用runYawEstimatorPrediction()更新加速度计和陀螺仪对yaw的估计, 对齐姿态角\n调用SelectMagFusion()选择融合用的磁力计数据, 开始也是通过yaw source来作为姿态角的一个参照, 调用fuseEulerYaw()融合yaw并计算bias, 然后读取磁力计数据\n**SelectVelPosFusion()**开始融合速度和位置数据, 调用CorrectGPSForAntennaOffset()上一时刻的速度减去用delta角度除以时间得到当前时刻的gps速度, 同理得到z轴高度\n// 判断gps是否可用, pose source是否为gps if (gpsDataToFuse \u0026amp;\u0026amp; (PV_AidingMode == AID_ABSOLUTE) \u0026amp;\u0026amp; (posxy_source == AP_NavEKF_Source::SourceXY::GPS)) { // 判断是否融合gps的pose或vel fuseVelData = frontend-\u0026gt;sources.useVelXYSource(AP_NavEKF_Source::SourceXY::GPS); fusePosData = true; #if EK3_FEATURE_EXTERNAL_NAV extNavUsedForPos = false; #endif // 把gps的有延迟的观测数据复制出来 if (fuseVelData) { velPosObs[0] = gpsDataDelayed.vel.x; velPosObs[1] = gpsDataDelayed.vel.y; velPosObs[2] = gpsDataDelayed.vel.z; } const Location gpsloc{gpsDataDelayed.lat, gpsDataDelayed.lng, 0, Location::AltFrame::ABSOLUTE}; const Vector2F posxy = EKF_origin.get_distance_NE_ftype(gpsloc); velPosObs[3] = posxy.x; velPosObs[4] = posxy.y; #if EK3_FEATURE_EXTERNAL_NAV } else if (extNavDataToFuse \u0026amp;\u0026amp; (PV_AidingMode == AID_ABSOLUTE) \u0026amp;\u0026amp; (posxy_source == AP_NavEKF_Source::SourceXY::EXTNAV)) { // 如果启用了视觉的话观测数据就换成视觉传来的数据 extNavUsedForPos = true; fusePosData = true; velPosObs[3] = extNavDataDelayed.pos.x; velPosObs[4] = extNavDataDelayed.pos.y; #endif // EK3_FEATURE_EXTERNAL_NAV } 得到数据之后先调用selectHeightForFusion()选取要融合的高度source, 这个函数前半部分也都是在判断读取气压计, gps或其他的数据, 计算传感器测量数据与ekf估计之间的偏移. 在gps数据可以融合的时候用贝叶斯滤波融合gps的数据到ekf的数据和方差里\n然后将数据放到ekf的观测里, 然后判断如果用gps并且可用并且需要reset的话, 把ekf的状态量的位置改为gps测得的数据\n然后又是一堆判断之后调用了最重要的**FuseVelPosNED()**来融合数据\n先判断赋值观测误差, 然后计算ekf的状态量与观测值之间的差值用于判断是否丢帧\n如果可以融合数据的话就先计算新息\n待更新\n","date":"May 23, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/study/apm_ekf3/","series":[],"smallImg":"","tags":[{"title":"ekf","url":"/tags/ekf/"}],"timestamp":1684829685,"title":"APM中ekf3算法"},{"authors":[],"categories":[{"title":"Multi-Sensor","url":"/categories/multi-sensor/"}],"content":"libraries/AP_VisualOdom/AP_VisualOdom.h 参数有相机的类别, 相机相对飞控的位移, 相机相对飞控摆放的方向(前后左右向下), 相机数据的缩放系数, 延迟, 速度\\位置\\yaw的噪声\nfloat posErr = 0; float angErr = 0; if (!isnan(m.pose_covariance[0])) { posErr = cbrtf(sq(m.pose_covariance[0])+sq(m.pose_covariance[6])+sq(m.pose_covariance[11])); angErr = cbrtf(sq(m.pose_covariance[15])+sq(m.pose_covariance[18])+sq(m.pose_covariance[20])); } // 噪声直接是将协方差矩阵中的相加 主要函数有\ninit()\n// create backend switch (VisualOdom_Type(_type.get())) { case VisualOdom_Type::None: // do nothing break; #if AP_VISUALODOM_MAV_ENABLED case VisualOdom_Type::MAV: _driver = new AP_VisualOdom_MAV(*this); break; #endif #if AP_VISUALODOM_INTELT265_ENABLED case VisualOdom_Type::IntelT265: case VisualOdom_Type::VOXL: _driver = new AP_VisualOdom_IntelT265(*this); break; #endif } 如果是t265的话默认成VOXL, 这两个应该用的是类似的数据\nhandle_vision_position_delta_msg()\n调用_driver-\u0026gt;handle_vision_position_delta_msg()获取角度和位置数据\nhandle_vision_position_estimate()\n获取四元数或者三轴角度然后调用_driver-\u0026gt;handle_vision_position_estimate()进行位置估计\nlibraries/AP_VisualOdom/AP_VisualOdom_IntelT265.cpp handle_vision_position_estimate()\nhandle_voxl_camera_reset_jump(pos, att, reset_counter); // 先调用判断传来的数据有没有跳变, 如果有跳变的话就重设参数, 该参数是从mavlink接受来的, 并不是飞控自己判断的 // 然后直接将获取到的位姿发送给导航或传感器等 rotate_and_correct_position(pos); rotate_attitude(att); // 矫正位姿 handle_vision_speed_estimate()\n// rotate velocity to align with vehicle Vector3f vel_corrected = vel; rotate_velocity(vel_corrected) 关于上位机传来的数据并没有怎么处理, 仅仅将位移和速度矫正了一下\n","date":"May 15, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/study/apm_t265/","series":[],"smallImg":"","tags":[{"title":"t265","url":"/tags/t265/"}],"timestamp":1684138485,"title":"APM中t265数据解析"},{"authors":[],"categories":[{"title":"Multi-Sensor","url":"/categories/multi-sensor/"}],"content":"libraries/AP_OpticalFlow/AP_OpticalFlow.cpp 主要定义了init(), handle_msp(), update(), start_calibration()等几个函数\ninit()\n主要用于接收光流传感器的类型并定义backend, backend是libraries/AP_OpticalFlow/AP_OpticalFlow_Backend.h中定义的OpticalFlow_backend基类对象, 派生出了PX4Flow, MAVLINK等几个具体传感器类型的派生类, 每个派生中重载了这个类的update(), handle_msg()函数, 分别用于获取光流数据和更新\nupdate()\n函数中主要调用了backend-\u0026gt;update()的函数, 之后_calibrator-\u0026gt;update()校准函数, _calibrator定义于start_calibration()中\nif (_calibrator != nullptr) { if (_calibrator-\u0026gt;update()) { // apply new calibration values const Vector2f new_scaling = _calibrator-\u0026gt;get_scalars(); const float flow_scalerx_as_multiplier = (1.0 + (_flowScalerX * 0.001)) * new_scaling.x; const float flow_scalery_as_multiplier = (1.0 + (_flowScalerY * 0.001)) * new_scaling.y; _flowScalerX.set_and_save_ifchanged((flow_scalerx_as_multiplier - 1.0) * 1000.0); _flowScalerY.set_and_save_ifchanged((flow_scalery_as_multiplier - 1.0) * 1000.0); _flowScalerX.notify(); _flowScalerY.notify(); GCS_SEND_TEXT(MAV_SEVERITY_INFO, \u0026#34;FlowCal: FLOW_FXSCALER=%d, FLOW_FYSCALER=%d\u0026#34;, (int)_flowScalerX, (int)_flowScalerY); } } _flowScalerX, _flowScalerY是地面站设置中的缩放参数. 这段代码用于更新缩放参数并发送给GCS\nstart_calibration()\nif (_calibrator == nullptr) { _calibrator = new AP_OpticalFlow_Calibrator(); if (_calibrator == nullptr) { GCS_SEND_TEXT(MAV_SEVERITY_CRITICAL, \u0026#34;FlowCal: failed to start\u0026#34;); return; } } if (_calibrator != nullptr) { _calibrator-\u0026gt;start(); } 用于给_calibrator赋值\nlibraries/AP_OpticalFlow/AP_OpticalFlow_MAV.cpp 主要派生了OpticalFlow_backend类, 重载了获取数据和更新用的handle_msg() update()函数, 新增了几个用于计算的变量\nhandle_msg()\nmavlink_msg_optical_flow_decode(\u0026amp;msg, \u0026amp;packet); latest_frame_us = AP_HAL::micros64(); flow_sum.x += packet.flow_x; flow_sum.y += packet.flow_y; quality_sum += packet.quality; count++; 从飞控读取数据后将这段时间的光流数据累加起来, 后续update时用的光流速度实际上是i这段时间的平均值\nupdate()\nif (gyro_sum_count \u0026lt; 1000) { const Vector3f\u0026amp; gyro = AP::ahrs().get_gyro(); gyro_sum.x += gyro.x; gyro_sum.y += gyro.y; gyro_sum_count++; } if (is_positive(dt) \u0026amp;\u0026amp; (dt \u0026lt; OPTFLOW_MAV_TIMEOUT_SEC)) { // calculate flow values const float flow_scale_factor_x = 1.0f + 0.001f * _flowScaler().x; const float flow_scale_factor_y = 1.0f + 0.001f * _flowScaler().y; // copy flow rates to state structure state.flowRate = { ((float)flow_sum.x / count) * flow_scale_factor_x * dt, ((float)flow_sum.y / count) * flow_scale_factor_y * dt }; // copy average body rate to state structure state.bodyRate = { gyro_sum.x / gyro_sum_count, gyro_sum.y / gyro_sum_count }; // we only apply yaw to flowRate as body rate comes from AHRS _applyYaw(state.flowRate); } else { // first frame received in some time so cannot calculate flow values state.flowRate.zero(); state.bodyRate.zero(); } 首先读取了imu的数据然后也做了平均数, 光流中的flowRate就是光流速度平均值乘以缩放系数, bodyRate是imu的平均值\nlibraries/AP_OpticalFlow/AP_OpticalFlow_Calibrator.cpp 主要用于计算最佳的缩放比例\nstruct sample_t { float flow_rate;\t// 光流速度 float body_rate;\t// imu速度 float los_pred;\t//ekf估计的在车辆运动方向的光流速度的分量 }; update()\nif (_cal_state == CalState::RUNNING) { uint32_t now_ms = AP_HAL::millis(); uint32_t timestamp_ms; Vector2f flow_rate, body_rate, los_pred; if (AP::ahrs().getOptFlowSample(timestamp_ms, flow_rate, body_rate, los_pred)) { add_sample(timestamp_ms, flow_rate, body_rate, los_pred); // while collecting samples display percentage complete if (now_ms - _last_report_ms \u0026gt; AP_OPTICALFLOW_CAL_STATUSINTERVAL_SEC * 1000UL) { _last_report_ms = now_ms; GCS_SEND_TEXT(MAV_SEVERITY_INFO, \u0026#34;%s x:%d%% y:%d%%\u0026#34;, prefix_str, (int)((_cal_data[0].num_samples * 100.0 / AP_OPTICALFLOW_CAL_MAX_SAMPLES)), (int)((_cal_data[1].num_samples * 100.0 / AP_OPTICALFLOW_CAL_MAX_SAMPLES))); } // advance state once sample buffers are full if (sample_buffers_full()) { _cal_state = CalState::READY_TO_CALIBRATE; GCS_SEND_TEXT(MAV_SEVERITY_INFO, \u0026#34;%s samples collected\u0026#34;, prefix_str); } } // check for timeout if (now_ms - _start_time_ms \u0026gt; AP_OPTICALFLOW_CAL_TIMEOUT_SEC * 1000UL) { GCS_SEND_TEXT(MAV_SEVERITY_INFO, \u0026#34;%s timeout\u0026#34;, prefix_str); _cal_state = CalState::FAILED; } } // start calibration if (_cal_state == CalState::READY_TO_CALIBRATE) { // run calibration and mark failure or success if (run_calibration()) { _cal_state = CalState::SUCCESS; return true; } else { _cal_state = CalState::FAILED; } } 先从AHRS中获取时间辍, 光流的速度和机体imu的速度, 然后运行run_calibration()进行校准\nrun_calibration()\n// calculate total absolute residual from all samples float total_abs_residual = 0; for (uint8_t i = 0; i \u0026lt; num_samples; i++) { const sample_t\u0026amp; samplei = _cal_data[axis].samples[i]; total_abs_residual += fabsf(calc_sample_residual(samplei, 1.0)); } // 此时计算的是系数为1, 得到的也就是实际上的光流速度 // 先计算残差和 // 如果残差和为零的话就返回1, 说明校准成功 // for each sample calculate the residual and scalar that best reduces the residual float best_scalar_total = 0; for (uint8_t i = 0; i \u0026lt; num_samples; i++) { float sample_best_scalar; const sample_t\u0026amp; samplei = _cal_data[axis].samples[i]; if (!calc_sample_best_scalar(samplei, sample_best_scalar)) { // failed to find the best scalar for a single sample // this should never happen because of checks when capturing samples GCS_SEND_TEXT(MAV_SEVERITY_INFO, \u0026#34;%s failed because of zero flow rate\u0026#34;, prefix_str); INTERNAL_ERROR(AP_InternalError::error_t::flow_of_control); return false; } const float sample_residual = calc_sample_residual(samplei, 1.0); best_scalar_total += sample_best_scalar * fabsf(sample_residual) / total_abs_residual; // 计算的是光流补偿量除以光流速度的平均值, 即系数的平均值 } // 最佳残差时的系数, 判断光流速度是否存在零, 如果存在零的话就返回false // 当计算出的最佳缩放系数的均方残差, 当均方小于系数为1计算出的均方残差时返回true (不太理解, 只要系数小于1的话均方肯定也是比他小的呀) // 返回的缩放系数为best_scalar_total calc_sample_residual()\nfloat AP_OpticalFlow_Calibrator::calc_sample_residual(const sample_t\u0026amp; sample, float scalar) const { return (sample.body_rate + ((sample.flow_rate * scalar) - sample.los_pred)); } 计算的残差为$imu速度+(光流速度*缩放系数-EKF估计的光流速度在车辆运动方向的分量)$, (等于0时说明系数最佳)\ncalc_sample_best_scalar()\n{ // if sample\u0026#39;s flow_rate is zero scalar has no effect // this should never happen because samples should have been checked before being added if (is_zero(sample.flow_rate)) { return false; } best_scalar = (sample.los_pred - sample.body_rate) / sample.flow_rate; return true; } $最佳系数=(EKF估计的光流速度在运动方向的分量-imu速度)/光流速度$\n","date":"May 14, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/study/apm_optical_flow/","series":[],"smallImg":"","tags":[{"title":"OpticalFlow","url":"/tags/opticalflow/"}],"timestamp":1684052085,"title":"APM光流传感器解析"},{"authors":[],"categories":[{"title":"GPS","url":"/categories/gps/"}],"content":"编译报错 /home/tioeare/project/FASTLAB/gps_ws/src/ublox/ublox_gps/src/node.cpp:39:10: fatal error: rtcm_msgs/Message.h: No such file or directory 39 | #include \u0026lt;rtcm_msgs/Message.h\u0026gt; | ^~~~~~~~~~~~~~~~~~~~~ compilation terminated. make[2]: *** [ublox/ublox_gps/CMakeFiles/ublox_gps_node.dir/build.make:76: ublox/ublox_gps/CMakeFiles/ublox_gps_node.dir/src/node.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:4154: ublox/ublox_gps/CMakeFiles/ublox_gps_node.dir/all] Error 2 make: *** [Makefile:146: all] Error 2 Invoking \u0026#34;make -j8 -l8\u0026#34; failed 下载https://github.com/tilk/rtcm_msgs放到gps_ws/src目录下\n/usr/bin/ld: CMakeFiles/ublox_gps_node.dir/src/node.cpp.o: in function `ublox_node::UbloxNode::configureUblox()\u0026#39;: node.cpp:(.text+0x86b7): undefined reference to `ublox_node::UbloxNode::kResetWait\u0026#39; /usr/bin/ld: /home/tioeare/project/FASTLAB/gps_ws/devel/lib/libublox_gps.so: undefined reference to `ublox_gps::Gps::kSetBaudrateSleepMs\u0026#39; collect2: error: ld returned 1 exit status make[2]: *** [ublox/ublox_gps/CMakeFiles/ublox_gps_node.dir/build.make:151: /home/tioeare/project/FASTLAB/gps_ws/devel/lib/ublox_gps/ublox_gps] Error 1 make[1]: *** [CMakeFiles/Makefile2:4505: ublox/ublox_gps/CMakeFiles/ublox_gps_node.dir/all] Error 2 make: *** [Makefile:146: all] Error 2 Invoking \u0026#34;make -j8 -l8\u0026#34; failed 将src/ublox/ublox_gps/include/ublox_gps/gps.h和src/ublox/ublox_gps/src/node.cpp中constexpr static int kSetBaudrateSleepMs = 500;和constexpr static int kResetWait = 10;移到Gps或UbloxNode类外\n","date":"May 5, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/project/ublox/","series":[],"smallImg":"","tags":[{"title":"GPS","url":"/tags/gps/"},{"title":"ros","url":"/tags/ros/"}],"timestamp":1683274485,"title":"使用ublox F9p GPS模块"},{"authors":[],"categories":[{"title":"Multi-Sensor","url":"/categories/multi-sensor/"}],"content":"apm坐标系 相机相比无人机中心:\nx正方向朝前\ny正方向朝右\nz正方向朝下\n数据格式 GPS 经度，维度，高度的绝对值\ngps获取到的数据的协方差(正常室外时协方差\u0026lt;1.5)\ntopic: /gps/fix\ntype: sensor_msgs/NavSatFix\nuint8 COVARIANCE_TYPE_UNKNOWN=0 uint8 COVARIANCE_TYPE_APPROXIMATED=1 uint8 COVARIANCE_TYPE_DIAGONAL_KNOWN=2 uint8 COVARIANCE_TYPE_KNOWN=3 std_msgs/Header header uint32 seq time stamp string frame_id sensor_msgs/NavSatStatus status int8 STATUS_NO_FIX=-1 int8 STATUS_FIX=0 int8 STATUS_SBAS_FIX=1 int8 STATUS_GBAS_FIX=2 uint16 SERVICE_GPS=1 uint16 SERVICE_GLONASS=2 uint16 SERVICE_COMPASS=4 uint16 SERVICE_GALILEO=8 int8 status uint16 service float64 latitude float64 longitude float64 altitude float64[9] position_covariance uint8 position_covariance_type 光流 光流x，y轴速度及竖直方向上相对地面的高度\n信号强度，光流质量\n# 自定义光流数据 std_msgs/Header header uint32 seq time stamp string frame_id float32 distance uint8 strength uint8 precision uint8 tof_status float32 flow_vel_x float32 flow_vel_y uint8 flow_quality uint8 flow_status # 板载imu数据 std_msgs/Header header uint32 seq time stamp string frame_id geometry_msgs/Quaternion orientation float64 x float64 y float64 z float64 w float64[9] orientation_covariance geometry_msgs/Vector3 angular_velocity float64 x float64 y float64 z float64[9] angular_velocity_covariance geometry_msgs/Vector3 linear_acceleration float64 x float64 y float64 z float64[9] linear_acceleration_covariance t265 相机当前坐标(相对于开机时的坐标)及姿态四元数\n坐标为相对坐标, 姿态四元数为绝对值\ntopic: /camera/odom/sample\ntype: nav_msgs/Odometry\nstd_msgs/Header header uint32 seq time stamp string frame_id string child_frame_id geometry_msgs/PoseWithCovariance pose geometry_msgs/Pose pose geometry_msgs/Point position float64 x float64 y float64 z geometry_msgs/Quaternion orientation float64 x float64 y float64 z float64 w float64[36] covariance geometry_msgs/TwistWithCovariance twist geometry_msgs/Twist twist geometry_msgs/Vector3 linear float64 x float64 y float64 z geometry_msgs/Vector3 angular float64 x float64 y float64 z float64[36] covariance 实现步骤 1. 获取数据 t265与gps通过ros订阅话题读取，光流通过串口读取\ngps数据需要与第一帧相减得到相对位移\n将t265的位姿用tf转换到机体坐标系, 光流与gps同样需要做转化\n光流数据还需要提前进行补偿，用imu或t265得到的位姿角度补偿光流速度\n将转化后的数据再与自己的前一帧数据做对比转化, 然后发布光流, t265, gps相对于自己前一帧的tf数据\n在滤波中再进行降噪和故障诊断\n故障诊断中, 光流可以用光流质量, 光流状态(待测), gps用协方差, t265用协方差(0.01时不可用, 0.001时可能出现漂移) 和光流/imu(imu没有给速度, 但t265却认为移动的情况) 单独判断\n滤波 递推加权最小二乘(WRLS) 最小二乘法得到多个传感器与真实值之间误差最小的权重, 再以该权重对多传感器数据进行加权\n$\\hat{X}=\\sum^n_{i=1}a_iZ_i$, $a_i$是$i$个传感器的加权系数, $Z_i$为每个传感器的测量值, 需满足 $$ E[\\hat{x}]=X=E[\\sum^n_{i=1}a_iX]+E[\\sum^n_{i=1}a_iV_i]=X\\sum^n_{i=1}a_i $$ $V_i$为噪声, 均值为0, 方差为$\\sigma^2$\n均方误差满足 $$ E[(X-\\hat{X})^2]=E[X-\\sum^n_{i=1}a_1(X+V_i)]^2=\\sum^n_{i=1}a^2_i\\sigma^2_i $$ 构造拉格朗日函数 $$ f(a_1,\u0026hellip;a_n,\\lambda)=\\sum^n_{i=1}a^2_i\\sigma^2_i-\\lambda(\\sum^n_{i=1}-1) $$ 求得 $$ a_i=\\frac{\\sigma^{-2}{i}}{\\sum^n{i=1}\\sigma^{-2}_{i}} $$\nEKF 将WRLS融合出的数据作为测量值, 无人机真实位置作为状态量, 三种传感器获取到的数据作为观测值 $$ X_{k|k-1} =A_{k|k-1}*X_{k-1} \\ P_{k|k-1}=A_{k|k-1}P_{k-1}A_{k|k-1}^T+RQR^T \\ K_k=P_{k|k-1}H^T_k(H_kP_{k|k-1}H^T_k+P_k)^{-1} \\ X=X_{k-1}+K_k(Z_k-H_kX_{k|k-1}) $$ Z为WRLS中融合得到的值\n滤波监测 对EKF输出的值进行检查，避免EKF结果过于发散\n室内x, y轴方向光流和imu互补滤波, 然后与EKF的数据进行对比, 相差过大时用互补滤波数据替代 GPS监测, 有GPS信号时根据GPS与imu互补滤波, 相差过大时进行复位, 对轨迹进行优化 室内回环检测, 鱼眼相机检测到回环的时候对目前的位姿进行优化 发布数据用以监测参数 光流原始数据(接收到的, 无需另外发布) /data_fusion/OF_data/flow_vel_x(y)\nimu数据 /mavros/imu/data/angular_velocity/x(y)\n光流补偿后的数据 /data_fusion/compensated_OF/point/x(y)\n融合前的t265, 光流, gps累加位移信息(光流需要单独积分然后发布)\n/data_fusion/oriUAV_t265/point/x(y, z)\n/data_fusion/OF_sum/point/x(y, z)\n/data_fusion/oriUAV_gps/point/x(y, z)\n用于融合的t265, 光流, gps的帧间数据\n/data_fusion/origin_dOF/point/x(y, z)\n/data_fusion/origin_dt265/point/x(y, z)\n/data_fusion/origin_dgps/point/x(y, z)\n融合后的帧间数据 /data_fusion/fusion_displace/point/x(y, z)\n融合后累加起来的位移 /data_fusion/pose_msg/pose/position/x(y, z)\ntype: geometry_msgs::PointStamped\nstd_msgs/Header header uint32 seq time stamp string frame_id geometry_msgs/Point position float64 x float64 y float64 z # 光流补偿效果 rqt_plot /data_fusion/OF_data/flow_vel_x /mavros/imu/data/angular_velocity/x /data_fusion/compensated_OF/point/x rqt_plot /data_fusion/OF_data/flow_vel_y /mavros/imu/data/angular_velocity/y /data_fusion/compensated_OF/point/y # 帧间数据融合效果 rqt_plot /data_fusion/origin_dOF/point/x /data_fusion/origin_dt265/point/x /data_fusion/origin_dgps/point/x /data_fusion/fusion_displace/point/x rqt_plot /data_fusion/origin_dOF/point/y /data_fusion/origin_dt265/point/y /data_fusion/origin_dgps/point/y /data_fusion/fusion_displace/point/y rqt_plot /data_fusion/origin_dOF/point/z /data_fusion/origin_dt265/point/z /data_fusion/origin_dgps/point/z /data_fusion/fusion_displace/point/z # 融合pose效果 rqt_plot /data_fusion/oriUAV_t265/point/x /data_fusion/OF_sum/point/x /data_fusion/oriUAV_gps/point/x /mavros/vision_pose/pose/pose/position/x rqt_plot /data_fusion/oriUAV_t265/point/y /data_fusion/OF_sum/point/y /data_fusion/oriUAV_gps/point/y /mavros/vision_pose/pose/pose/position/y rqt_plot /data_fusion/oriUAV_t265/point/z /data_fusion/OF_sum/point/z /data_fusion/oriUAV_gps/point/z /mavros/vision_pose/pose/pose/position/z BUG mavros编译 CMake Error: File /home/tioeare/project/FASTLAB/mavros_ws/src/mavlink/config.h.in does not exist. CMake Error at CMakeLists.txt:68 (configure_file): configure_file Problem configuring file catkin_ws/src/mavlink下新建config.h.in文件\n写入\n#define MAVLINK_VERSION \u0026#34;${PROJECT_VERSION}\u0026#34; 重新编译\nmavros读不到imu等的数据 先连一下qgc地面站\n局域网ros export ROS_HOSTNAME=tioeare export ROS_MASTER_URI=http://192.168.112.86:11311 ","date":"May 5, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/project/multi-sensor-fusion/","series":[],"smallImg":"","tags":[{"title":"GPS","url":"/tags/gps/"},{"title":"ros","url":"/tags/ros/"}],"timestamp":1683274485,"title":"空地无人机GPS与T265，光流传感器等的数据融合"},{"authors":[],"categories":[],"content":"Ardupilot补偿光流 问题概述 主要问题: 光流传感器假定无人机为完全水平飞行, 当无人机姿态发生微小倾斜时, 光流认为此时无人机有位移, 进而输出该位移数据, 导致Loiter模式下出现左右浮动\n衍生问题: 1.光流模块并不在无人机旋转中心, 当无人机位姿发生变化时, 光流模块的旋转角速度与imu的角速度不一定相同. 2.光流数据的频率与imu发布频率不一定相同, 中间可能出现时间差影响补偿效果\n融合方案 代码修改 获取光流得到的x轴和y轴速度 获取加速度计得到的pitch和roll轴速度 将加速度计得到的数据转化为光流坐标系下的速度 光流速度减去加速度计速度即可 参考[4], [5]中的代码\n///参考资料[4] Flow_Decode(\u0026amp;Flow_Buffer[0]);\t// 光流数据解析 TimePeriod(\u0026amp;flow_Delta);\t// 得到这段代码运行时间ms Flow_Delta_T = Flow_Delta.Time_Delta/1000.0f;\t// 得到周期时间 // 得到陀螺仪数据角速度(弧度) gyro_y = imu.gyroRaw[1] * 180.0f / M_PI_F;\t// gyro[1] pitch光流Y轴 gyro_x = imu.gyroRaw[0] * 180.0f / M_PI_F;\t// gyro[0] roll光流X轴 // 陀螺仪滤波 LPF_1_(8.0f, Flow_Delta_T, gyro_y, gyro_lpf_y);\t// gyro low pass filter (delay) for fitting flow data() LPF_1_(8.0f, Flow_Delta_T, gyro_x, gyro_lpf_x);\t// 8.0需要调参, 保证光流数值和陀螺仪数值相位相差不多 // 对光流原始数据进行滤波 LPF_1_(30.0f, Flow_Delta_T, flow_dat.x, flow_dat.x);\t// 对光流数据进行简单滤波 LPF_1_(30.0f, Flow_Delta_T, flow_dat.y, flow_dat.y); // 速度计算方法: (H * flow_dat.x / T) * 100 H为高度, 单位m. T为间隔时间, 单位s, 100是m-\u0026gt;cm的转换 UPflow_speed_x = flow_dat.x / Flow_Delta_T ; // 速度 rad/s UPflow_speed_y = flow_dat.y / Flow_Delta_T ; // 速度 rad/s /* --------------------利用陀螺仪对光流进行补偿，保证在原地旋转时，光流输出几乎为 0 -----------------*/ // 1.105和1.101系数需要调参, 用来抵消低通滤波带来的幅值减小 UPflow_speed_x = US100_Distance * (UPflow_speed_x - 1.105 * LIMIT(((gyro_lpf_x) / 57.295779f), -flow_t2, flow_t2)) * 100.0f;\t// 速度 cm/s UPflow_speed_y = US100_Distance * (UPflow_speed_y + 1.101 * LIMIT(((gyro_lpf_y) / 57.295779f), -flow_t1, flow_t1)) * 100.0f; 其中Flow_Decode(), LPF_1_(), LIMIT()函数由该博主使用的光流模块厂家提供, 用于获取光流速度和滤波等\n/// 参考资料[5] if (get_flow_quality() != 0) { flow_angle_fix_x = +(120.0f * tan(pitch_delay[3])); flow_angle_fix_y = -(120.0f * tan(roll_delay[3])); } else { flow_angle_fix_x = 0; flow_angle_fix_y = 0; } flow_cal_distance_x = flow_ori_distance_x - flow_angle_fix_x; // cm flow_cal_distance_y = flow_ori_distance_y - flow_angle_fix_y; // cm float flow_speed_x = (flow_cal_distance_x - last_distance_x) / 0.02f; float flow_speed_y = (flow_cal_distance_y - last_distance_y) / 0.02f; last_distance_x = flow_cal_distance_x; last_distance_y = flow_cal_distance_y; 以上两篇中主要思想都是通过光流的模块的相关函数获取到光流的数据, 然后将该数据乘以一个需要自己调整的参数, 然后再减去从imu获得的数据.\n不同点是[4]中直接获取的光流速度然后利用他的模块手册提供的滤波函数对数据进行滤波之后再减去imu补偿量, 得到补偿后的速度. [5]中是先获取速度然后积分成位移, 再用位移减去补偿量然后再微分成速度.(这么做主要考虑到速度不好标定, 但位移相对来说更容易得到测量值用于矫正参数的赋值)\n目前使用的MTF-01光流传感器只提供了读取光流速度的相关函数, 如下\n// mtf01.h #pragma once #include \u0026lt;stdbool.h\u0026gt; #include \u0026lt;stdint.h\u0026gt; #include \u0026lt;string.h\u0026gt; #define MICOLINK_MSG_HEAD 0xEF #define MICOLINK_MAX_PAYLOAD_LEN 64 #define MICOLINK_MAX_LEN MICOLINK_MAX_PAYLOAD_LEN + 7 /* 消息ID定义 */ enum { MICOLINK_MSG_ID_RANGE_SENSOR = 0x51, // 测距传感器 }; /* 消息结构体定义 */ typedef struct { uint8_t head; uint8_t dev_id; uint8_t sys_id; uint8_t msg_id; uint8_t seq; uint8_t len; uint8_t payload[MICOLINK_MAX_PAYLOAD_LEN]; uint8_t checksum; uint8_t status; uint8_t payload_cnt; } MICOLINK_MSG_t; /* 数据负载定义 */ #pragma pack(1) // 测距传感器 typedef struct { uint32_t time_ms; // 系统时间 ms uint32_t distance; // 距离(mm) 最小值为10，0表示数据不可用 uint8_t strength; // 信号强度 uint8_t precision; // 精度 uint8_t tof_status; // 状态 uint8_t reserved1; // 预留 int16_t flow_vel_x; // 光流速度x轴 int16_t flow_vel_y; // 光流速度y轴 uint8_t flow_quality; // 光流质量 uint8_t flow_status; // 光流状态 uint16_t reserved2; // 预留 } MICOLINK_PAYLOAD_RANGE_SENSOR_t; #pragma pack() // mtf01.c #include \u0026#34;mtf01.h\u0026#34; /* 说明： 用户使用micolink_decode作为串口数据处理函数即可 距离有效值最小为10(mm),为0说明此时距离值不可用 光流速度值单位：cm/s@1m 飞控中只需要将光流速度值*高度，即可得到真实水平位移速度 计算公式：实际速度(cm/s)=光流速度*高度(m) */ bool micolink_parse_char(MICOLINK_MSG_t *msg, uint8_t data); void micolink_decode(uint8_t data) { static MICOLINK_MSG_t msg; if (micolink_parse_char(\u0026amp;msg, data) == false) return; switch (msg.msg_id) { case MICOLINK_MSG_ID_RANGE_SENSOR: { MICOLINK_PAYLOAD_RANGE_SENSOR_t payload; memcpy(\u0026amp;payload, msg.payload, msg.len); /* 此处可获取传感器数据: 距离 = payload.distance; 强度 = payload.strength; 精度 = payload.precision; 距离状态 = payload.tof_status; 光流速度x轴 = payload.flow_vel_x; 光流速度y轴 = payload.flow_vel_y; 光流质量 = payload.flow_quality; 光流状态 = payload.flow_status; */ break; } default: break; } } bool micolink_check_sum(MICOLINK_MSG_t *msg) { uint8_t length = msg-\u0026gt;len + 6; uint8_t temp[MICOLINK_MAX_LEN]; uint8_t checksum = 0; memcpy(temp, msg, length); for (uint8_t i = 0; i \u0026lt; length; i++) { checksum += temp[i]; } if (checksum == msg-\u0026gt;checksum) return true; else return false; } bool micolink_parse_char(MICOLINK_MSG_t *msg, uint8_t data) { switch (msg-\u0026gt;status) { case 0: // 帧头 if (data == MICOLINK_MSG_HEAD) { msg-\u0026gt;head = data; msg-\u0026gt;status++; } break; case 1: // 设备ID msg-\u0026gt;dev_id = data; msg-\u0026gt;status++; break; case 2: // 系统ID msg-\u0026gt;sys_id = data; msg-\u0026gt;status++; break; case 3: // 消息ID msg-\u0026gt;msg_id = data; msg-\u0026gt;status++; break; case 4: // 包序列 msg-\u0026gt;seq = data; msg-\u0026gt;status++; break; case 5: // 负载长度 msg-\u0026gt;len = data; if (msg-\u0026gt;len == 0) msg-\u0026gt;status += 2; else if (msg-\u0026gt;len \u0026gt; MICOLINK_MAX_PAYLOAD_LEN) msg-\u0026gt;status = 0; else msg-\u0026gt;status++; break; case 6: // 数据负载接收 msg-\u0026gt;payload[msg-\u0026gt;payload_cnt++] = data; if (msg-\u0026gt;payload_cnt == msg-\u0026gt;len) { msg-\u0026gt;payload_cnt = 0; msg-\u0026gt;status++; } break; case 7: // 帧校验 msg-\u0026gt;checksum = data; msg-\u0026gt;status = 0; if (micolink_check_sum(msg)) { return true; } default: msg-\u0026gt;status = 0; msg-\u0026gt;payload_cnt = 0; break; } return false; } 推测读取光流速度后直接减去陀螺仪获取的速度然后赋给ek3进行后续的数据融合即可\nArdupilot官方文档中的校准传感器 见参考资料[6], 在文档中发现一出校准光流传感器的操作, 其中提到FLOW_FXSCALER, FLOW_FYSCALER两个参数用于把光流速度与imu速度调整到相近波形, 尚未经过测试这个参数的用处\n参考资料 [1] 光流定点若干问题分析_权盛光流 无法定高_engineerlixl的博客-CSDN博客\n[2] Ardupilot光流代码分析 - CodeAntenna\n[3] PX4FLOW光流模块DIY（含部分代码讲解）_xian_z的博客-CSDN博客\n[4] 无人机—加速度计与光流数据融合_光流融合_经纬的无疆的博客-CSDN博客\n[5] 无人机飞控之光流知识小结（包含LK算法的讲解，across写的） | 码农家园 (codenong.com)\n[6] Optical Flow Sensor Testing and Setup — Copter documentation (ardupilot.org)\n","date":"April 28, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/daily/2023.04.28/","series":[],"smallImg":"","tags":[],"timestamp":1682618565,"title":"Imu补偿光流"},{"authors":[],"categories":[{"title":"bug","url":"/categories/bug/"}],"content":"前两天下载了ardupilot的源码，在配置编译环境的时候用了官方提供的Tools/environment_install/install-prereqs-ubuntu.sh，结果好多报错。\n然后我去pkgs一个一个找apt安装失败的包，然后手动下载安装，遇到依赖问题就继续重新安装，到最后虽然成功安装好了，我一重启发现安装的时候好像给我桌面环境什么的自动卸掉了，想要sudo apt install ubuntu-desktop按回来，但是一直失败，提示我找不到这个包。我又wget下载了一个deb的包想用dpkg安装看看到底哪里有问题，发现确实是新安装的依赖的问题。\n最后换了一下源，我之前一直用的初始的main源，现在换清华源了之后一下就好了，自动把我的所有依赖都更新成适合的，成功apt安装回桌面\ndeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ focal-security main restricted universe multiverse ","date":"April 21, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/bug/apm%E7%BC%96%E8%AF%91%E7%8E%AF%E5%A2%83/","series":[],"smallImg":"","tags":[{"title":"bug","url":"/tags/bug/"}],"timestamp":1682051760,"title":"Ardupilot编译依赖项安装失败"},{"authors":[],"categories":[{"title":"bug","url":"/categories/bug/"}],"content":"之前一直在用clion，但是clion占用内存太多了，打开有点慢，并且同时运行几项目的话有点卡，补全和提示之类的功能延迟会变得很高甚至就没有了。\n然后想转到vscode只做为临时打开某个项目运行一下的编辑器，开始用了几天没遇到什么大问题，然后今天突然打开一个ros项目的时候，cmake报错找不到catkin。就很奇怪，不知道怎么回事，后来找到一个只能说是临时的解决方案，在setting.json中加一行\n\u0026#34;cmake.configureArgs\u0026#34;: [ \u0026#34;-DCMAKE_PREFIX_PATH=/opt/ros/noetic\u0026#34; ], 手动设置一下cmake的查找路径\n","date":"April 21, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/bug/vscode/","series":[],"smallImg":"","tags":[{"title":"bug","url":"/tags/bug/"}],"timestamp":1682051760,"title":"Vscode Cmake Find_package()找不到catkin"},{"authors":[],"categories":[],"content":"","date":"April 13, 2023","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/zh-hans/about/","series":[],"smallImg":"","tags":[],"timestamp":1681344000,"title":"关于我"},{"authors":[],"categories":[{"title":"bug","url":"/categories/bug/"}],"content":"ThunderrBird是ubuntu下自带的一个很好用的邮件软件，但是实际使用中总会遇到一些同步失败的情况，之前在windows下用outlook同步一些邮件的时候也会时常发生同步失败，并且由于没有那么经常用邮箱。而我的三星手机自带的邮件app又从未发生过同步失败，并没有很影响我的正常生活，所以之前也就一直没有管他。今天刚好又遇到了这个问题，就查了一下准备彻底解决他。\ngmail无法同步 即使挂了代理，也经常会出现同步失败，登陆验证不了的情况。\n最后查过之后发现是账户设置的问题，thunderbird会自动把gmail的登陆验证模式改为OAuth2\n需要在Saved Passwords\u0026hellip;中将imap，smtp开头的gamil密码删掉，只保留oauth开头的密码\noutlook同步失败 此外还发现一个比较奇怪的现象，outlook在开代理之后不论是登陆还是同步都会失败，代理关掉就正常了。按理来说微软的outlook国外ip不应该不能访问的呀。\n无奈，只能手动取消掉outlook的代理\n同步outlook日历 安装这三个插件并在TbSync中登陆outlook账号即可\n","date":"April 6, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/bug/thunderbird/","series":[],"smallImg":"","tags":[{"title":"bug","url":"/tags/bug/"}],"timestamp":1680710400,"title":"ThunderrBird同步"},{"authors":[],"categories":[{"title":"slam","url":"/categories/slam/"}],"content":"orb slam3稠密建图 项目介绍 开发环境 ubuntu 20.04\nros noetic\nOpenCV 4.5.5\npcl 1.13\norb_slam3_pcl_mapping ├── CMakeLists.txt ├── package.xml ├── README.md └── src ├── PointCloudMapper.cc ├── PointCloudMapper.h └── pointcloud_mapping.cpp 主要参照以下项目进行修改适配orb slam3与d455，并增加点云的回环\nhttps://blog.csdn.net/crp997576280/article/details/104220926\nhttps://github.com/xiaobainixi/ORB-SLAM2_RGBD_DENSE_MAP\n代码简介 PointCloudMapper.cc 头文件中主要定义了相关变量和函数\nvoid viewer(); 唯一一个由外部调用的函数，主要用于点云的拼接和显示，以及判断当前是否检测到回环\nwhile (ros::ok()) { ros::spinOnce(); //用于检测是否有关键帧加入 KFUpdate = false; { std::unique_lock\u0026lt;std::mutex\u0026gt; lck(keyframeMutex); N = mvGlobalPointCloudsPose.size(); KFUpdate = mbKeyFrameUpdate; mbKeyFrameUpdate = false; } //是否有关键帧加入或是否是回环模式 if ((KFUpdate \u0026amp;\u0026amp; N \u0026gt; lastKeyframeSize) || is_loop_for_remap) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock_loop(loopUpdateMutex); //如果是回环的话根据BA后的位姿重新绘制点云 if (is_loop_for_remap) { std::cout \u0026lt;\u0026lt; RED \u0026lt;\u0026lt; \u0026#34;detect loop!\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;mvGlobalPointCloudsPose size: \u0026#34; \u0026lt;\u0026lt; mvGlobalPointCloudsPose.size() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;depthImgs size: \u0026#34; \u0026lt;\u0026lt; depthImgs.size() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;colorImgs size: \u0026#34; \u0026lt;\u0026lt; colorImgs.size() \u0026lt;\u0026lt; std::endl; globalMap-\u0026gt;clear(); for (int i = 0; i \u0026lt; depthImgs.size(); i += 1) { tmp-\u0026gt;clear(); for (int m = 0; m \u0026lt; depthImgs[i].rows; m += 3) { for (int n = 0; n \u0026lt; depthImgs[i].cols; n += 3) { float d = depthImgs[i].ptr\u0026lt;float\u0026gt;(m)[n] / mDepthMapFactor; if (d \u0026lt; 0 || d \u0026gt; max_distance) { continue; } PointT p; p.z = d; p.x = (n - mcx) * p.z / mfx; p.y = (m - mcy) * p.z / mfy; p.r = colorImgs[i].data[m * colorImgs[i].step + n * colorImgs[i].channels()]; p.g = colorImgs[i].data[m * colorImgs[i].step + n * colorImgs[i].channels() + 1]; p.b = colorImgs[i].data[m * colorImgs[i].step + n * colorImgs[i].channels() + 2]; tmp-\u0026gt;points.push_back(p); } } cloud_voxel_tem-\u0026gt;clear(); tmp-\u0026gt;is_dense = false; voxel.setInputCloud(tmp); voxel.setLeafSize(mresolution, mresolution, mresolution); voxel.filter(*cloud_voxel_tem); cloud1-\u0026gt;clear(); pcl::transformPointCloud(*cloud_voxel_tem, *cloud1, mvGlobalPointCloudsPose[i].inverse().matrix()); *globalMap += *cloud1; } is_loop_for_remap = false; } else { //如果有新点云加入则拼接点云 PointCloud::Ptr tem_cloud1(new PointCloud()); std::cout \u0026lt;\u0026lt; GREEN \u0026lt;\u0026lt; \u0026#34;mvPosePointClouds.size(): \u0026#34; \u0026lt;\u0026lt; mvGlobalPointCloudsPose.size() \u0026lt;\u0026lt; std::endl; tem_cloud1 = generatePointCloud(colorImgs.back(), depthImgs.back(), mvGlobalPointCloudsPose.back()); if (tem_cloud1-\u0026gt;empty()) continue; *globalMap += *tem_cloud1; sensor_msgs::PointCloud2 local; pcl::toROSMsg(*tem_cloud1, local); local.header.stamp = ros::Time::now(); local.header.frame_id = \u0026#34;local\u0026#34;; pub_local_pointcloud.publish(local); } lastKeyframeSize = mvGlobalPointCloudsPose.size(); sensor_msgs::PointCloud2 output; pcl::toROSMsg(*globalMap, output); output.header.stamp = ros::Time::now(); output.header.frame_id = \u0026#34;world\u0026#34;; pub_global_pointcloud.publish(output); pcl_viewer.showCloud(globalMap); // std::cout \u0026lt;\u0026lt; WHITE \u0026lt;\u0026lt; \u0026#34;show global map, size=\u0026#34; \u0026lt;\u0026lt; globalMap-\u0026gt;points.size() \u0026lt;\u0026lt; std::endl; } } 增加线程锁是为了避免与后面的回环造成数据冲突，每有一个新的关键帧加入时，将关键帧对应的点云和rgb图像以及此时相机的位姿进行转化，转化到世界坐标系下的彩色点云\nvoid PointCloudMapper::callback(\u0026hellip;) 用于接受订阅消息的回调函数，接收到图像和位姿后将geometry_msgs::PoseStamped::ConstPtr的位姿转化为Eigen::Isometry3f类型，然后调用insertKeyFrame(...)将接收到的数据储存起来\n当检测到回环是，不再接受某一关键帧的位姿信息，而是接收BA后的相机轨迹，再将每一帧的轨迹存储到Eigen::Isometry3f的vector中\nif (is_loop) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock_loop(loopUpdateMutex); std::vector\u0026lt;Eigen::Isometry3f\u0026gt; poses; std::vector\u0026lt;cv::Mat\u0026gt; colors, depths; for (long i = 0; i \u0026lt; path-\u0026gt;poses.size(); i++) { for (long j = i; j \u0026lt; kf_ids.size(); j++) { if (kf_ids[j] == long(path-\u0026gt;poses[i].header.seq)) { geometry_msgs::PoseStamped Tcw = path-\u0026gt;poses[i]; Eigen::Affine3f affine; Eigen::Vector3f Oe; Oe(0) = Tcw.pose.position.x; Oe(1) = Tcw.pose.position.y; Oe(2) = Tcw.pose.position.z; affine.translation() = Oe; Eigen::Quaternionf q; q.x() = Tcw.pose.orientation.x; q.y() = Tcw.pose.orientation.y; q.z() = Tcw.pose.orientation.z; q.w() = Tcw.pose.orientation.w; Eigen::Matrix3f Re(q); affine.linear() = Re; affine.translation() = Oe; Eigen::Isometry3f T = Eigen::Isometry3f(affine.cast\u0026lt;float\u0026gt;().matrix()); poses.push_back(T); colors.push_back(colorImgs[j]); depths.push_back(depthImgs[j]); break; } } } is_loop = false; is_loop_for_remap = true; if (poses.empty()) return; mvGlobalPointCloudsPose.swap(poses); colorImgs.swap(colors); depthImgs.swap(depths); } else if (!is_loop_for_remap) { kf_ids.push_back(msgRGB-\u0026gt;header.seq); insertKeyFrame(color, depth, T); } void PointCloudMapper::boolCallback() 检测到回环时调用的回调函数\norb slam3修改部分 ros_rgbd.cc #include... using namespace std; class ImageGrabber { public: vector\u0026lt;unsigned long\u0026gt; key_frame_id; uint32_t pub_id = 0; ros::NodeHandle nh1; ros::Publisher pub_rgb, pub_depth, pub_tcw, pub_camerapath, pub_odom, pub_isLoop; nav_msgs::Path camerapath; ImageGrabber(ORB_SLAM3::System *pSLAM) : mpSLAM(pSLAM), nh1(\u0026#34;~\u0026#34;) { pub_rgb = nh1.advertise\u0026lt;sensor_msgs::Image\u0026gt;(\u0026#34;RGB/Image\u0026#34;, 1); pub_depth = nh1.advertise\u0026lt;sensor_msgs::Image\u0026gt;(\u0026#34;Depth/Image\u0026#34;, 1); pub_tcw = nh1.advertise\u0026lt;geometry_msgs::PoseStamped\u0026gt;(\u0026#34;CameraPose\u0026#34;, 1); pub_odom = nh1.advertise\u0026lt;nav_msgs::Odometry\u0026gt;(\u0026#34;Odometry\u0026#34;, 1); pub_camerapath = nh1.advertise\u0026lt;nav_msgs::Path\u0026gt;(\u0026#34;Path\u0026#34;, 1); pub_isLoop = nh1.advertise\u0026lt;std_msgs::Bool\u0026gt;(\u0026#34;isLoop\u0026#34;, 1); } // ImageGrabber(ORB_SLAM3::System *pSLAM) : mpSLAM(pSLAM) {} void GrabRGBD(const sensor_msgs::ImageConstPtr \u0026amp;msgRGB, const sensor_msgs::ImageConstPtr \u0026amp;msgD); ORB_SLAM3::System *mpSLAM; }; int main(int argc, char **argv) { ros::init(argc, argv, \u0026#34;RGBD\u0026#34;); ros::start(); if (argc != 3) { cerr \u0026lt;\u0026lt; endl \u0026lt;\u0026lt; \u0026#34;Usage: rosrun ORB_SLAM3 RGBD path_to_vocabulary path_to_settings\u0026#34; \u0026lt;\u0026lt; endl; ros::shutdown(); return 1; } // Create SLAM system. It initializes all system threads and gets ready to process frames. ORB_SLAM3::System SLAM(argv[1], argv[2], ORB_SLAM3::System::RGBD, true); ros::NodeHandle nh; ImageGrabber igb(\u0026amp;SLAM); message_filters::Subscriber\u0026lt;sensor_msgs::Image\u0026gt; rgb_sub(nh, \u0026#34;/camera/color/image_raw\u0026#34;, 10); message_filters::Subscriber\u0026lt;sensor_msgs::Image\u0026gt; depth_sub(nh, \u0026#34;/camera/aligned_depth_to_color/image_raw\u0026#34;, 10); typedef message_filters::sync_policies::ApproximateTime\u0026lt;sensor_msgs::Image, sensor_msgs::Image\u0026gt; sync_pol; message_filters::Synchronizer\u0026lt;sync_pol\u0026gt; sync(sync_pol(10), rgb_sub, depth_sub); sync.registerCallback(boost::bind(\u0026amp;ImageGrabber::GrabRGBD, \u0026amp;igb, _1, _2)); ros::spin(); // Stop all threads SLAM.Shutdown(); // Save camera trajectory SLAM.SaveKeyFrameTrajectoryTUM(\u0026#34;KeyFrameTrajectory.txt\u0026#34;); ros::shutdown(); return 0; } tf::Pose trans_pose(Sophus::SE3f se3, bool \u0026amp;if_empty) { cv::Mat Tcw = (cv::Mat_\u0026lt;float\u0026gt;(4, 4) \u0026lt;\u0026lt; se3.matrix()(0, 0), se3.matrix()(0, 1), se3.matrix()(0, 2), se3.matrix()(0, 3), se3.matrix()(1, 0), se3.matrix()(1, 1), se3.matrix()(1, 2), se3.matrix()(1, 3), se3.matrix()(2, 0), se3.matrix()(2, 1), se3.matrix()(2, 2), se3.matrix()(2, 3), 0.0f, 0.0f, 0.0f, 1.0f); if_empty = Tcw.empty(); cv::Mat RWC = Tcw.rowRange(0, 3).colRange(0, 3); cv::Mat tWC = Tcw.rowRange(0, 3).col(3); tf::Matrix3x3 M(RWC.at\u0026lt;float\u0026gt;(0, 0), RWC.at\u0026lt;float\u0026gt;(0, 1), RWC.at\u0026lt;float\u0026gt;(0, 2), RWC.at\u0026lt;float\u0026gt;(1, 0), RWC.at\u0026lt;float\u0026gt;(1, 1), RWC.at\u0026lt;float\u0026gt;(1, 2), RWC.at\u0026lt;float\u0026gt;(2, 0), RWC.at\u0026lt;float\u0026gt;(2, 1), RWC.at\u0026lt;float\u0026gt;(2, 2)); tf::Vector3 V(tWC.at\u0026lt;float\u0026gt;(0) / 25, tWC.at\u0026lt;float\u0026gt;(1) / 25, tWC.at\u0026lt;float\u0026gt;(2) / 25); tf::Quaternion q; M.getRotation(q); tf::Pose tf_pose(q, V); double roll, pitch, yaw; M.getRPY(roll, pitch, yaw); if (roll == 0 || pitch == 0 || yaw == 0) if_empty = true; return tf_pose; } void ImageGrabber::GrabRGBD(const sensor_msgs::ImageConstPtr \u0026amp;msgRGB, const sensor_msgs::ImageConstPtr \u0026amp;msgD) { // Copy the ros image message to cv::Mat. cv_bridge::CvImageConstPtr cv_ptrRGB; try { cv_ptrRGB = cv_bridge::toCvShare(msgRGB, sensor_msgs::image_encodings::BGR8); } catch (cv_bridge::Exception \u0026amp;e) { ROS_ERROR(\u0026#34;cv_bridge exception: %s\u0026#34;, e.what()); return; } cv_bridge::CvImageConstPtr cv_ptrD; try { cv_ptrD = cv_bridge::toCvShare(msgD); } catch (cv_bridge::Exception \u0026amp;e) { ROS_ERROR(\u0026#34;cv_bridge exception: %s\u0026#34;, e.what()); return; } if (cv_ptrRGB-\u0026gt;image.empty() || cv_ptrD-\u0026gt;image.empty()) return; Sophus::SE3f se3 = mpSLAM-\u0026gt;TrackRGBD(cv_ptrRGB-\u0026gt;image, cv_ptrD-\u0026gt;image, cv_ptrRGB-\u0026gt;header.stamp.toSec()); bool if_empty; tf::Pose tf_pose = trans_pose(se3, if_empty); if (!if_empty) { std_msgs::Header header; header.stamp = msgRGB-\u0026gt;header.stamp; header.seq = msgRGB-\u0026gt;header.seq; header.frame_id = \u0026#34;camera\u0026#34;; sensor_msgs::Image::ConstPtr rgb_msg = msgRGB; sensor_msgs::Image::ConstPtr depth_msg = msgD; geometry_msgs::PoseStamped tcw_msg; tcw_msg.header = header; tf::poseTFToMsg(tf_pose, tcw_msg.pose); camerapath.header = header; std_msgs::Bool isLoop_msg; isLoop_msg.data = mpSLAM-\u0026gt;is_loop; if (mpSLAM-\u0026gt;is_loop) { vector\u0026lt;geometry_msgs::PoseStamped\u0026gt; after_loop_poses; for (long i = 0; i \u0026lt; key_frame_id.size(); i++) { for (long j = 0; j \u0026lt; mpSLAM-\u0026gt;current_all_KF.size(); j++) { if (key_frame_id[i] == mpSLAM-\u0026gt;current_all_KF[j]-\u0026gt;mnId) { tf_pose = trans_pose(mpSLAM-\u0026gt;current_all_KF[j]-\u0026gt;GetPose(), if_empty); geometry_msgs::PoseStamped tcw_msg1; tf::poseTFToMsg(tf_pose, tcw_msg1.pose); tcw_msg1.header = header; tcw_msg1.header.seq = i; after_loop_poses.push_back(tcw_msg1); break; } } } camerapath.poses.swap(after_loop_poses); } if (mpSLAM-\u0026gt;is_loop) pub_isLoop.publish(isLoop_msg); if (mpSLAM-\u0026gt;is_key_frame) { key_frame_id.push_back(mpSLAM-\u0026gt;current_KF_mnId); pub_camerapath.publish(camerapath); pub_tcw.publish(tcw_msg); pub_rgb.publish(rgb_msg); pub_depth.publish(depth_msg); } } } 主要用于发布相关话题\nSystem.cc Sophus::SE3f System::TrackRGBD(const cv::Mat \u0026amp;im, const cv::Mat \u0026amp;depthmap, const double \u0026amp;timestamp, const vector\u0026lt;IMU::Point\u0026gt; \u0026amp;vImuMeas, string filename) { ... is_key_frame = false; is_key_frame = mpLocalMapper-\u0026gt;is_key_frame; is_loop = false; is_loop = mpTracker-\u0026gt;is_loop; current_all_KF = mpAtlas-\u0026gt;GetAllKeyFrames(); if (is_key_frame) current_KF_mnId = mpLocalMapper-\u0026gt;GetCurrKF()-\u0026gt;mnId; if (mpTracker-\u0026gt;is_loop){ std::unique_lock\u0026lt;std::mutex\u0026gt; lock_loop(mpTracker-\u0026gt;is_loop_mutex); mpTracker-\u0026gt;is_loop = false; } if (mpLocalMapper-\u0026gt;is_key_frame){ std::unique_lock\u0026lt;std::mutex\u0026gt; lock_kf(mpLocalMapper-\u0026gt;is_keyframe_mutex); mpLocalMapper-\u0026gt;is_key_frame = false; } } 主要增加了以上用于判断是否为关键帧，是否检测到回环的相关变量和判断，并获取此时所有的关键帧\nLocalMapping.cc void LocalMapping::Run() { if (CheckNewKeyFrames() \u0026amp;\u0026amp; !mbBadImu) { { std::unique_lock\u0026lt;std::mutex\u0026gt; lock_kf(is_keyframe_mutex); is_key_frame = true; } MapPointCulling(); ... } } 添加了判断是否为关键帧，注意要在构建LocalMapping中添加，因为BA时使用的是地图点，但好像并不是所有关键帧都在地图点中。如果在Track()中添加的话会导致keyFrame与优化后mpAtlas-\u0026gt;GetAllKeyFrames()返回的数目不一致\nLoopClosing.cc while(1){ ... if (mbLoopDetected) { ... { std::unique_lock\u0026lt;std::mutex\u0026gt; lock_loop(mpTracker-\u0026gt;is_loop_mutex); mpTracker-\u0026gt;is_loop = true; } mpLoopLastCurrentKF-\u0026gt;SetErase(); ... } } 同样添加用于判断是否检测到了回环\n运行效果 bilibili\n使用intel relsense d455回绕一圈后构建的点云图\n","date":"April 3, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/project/orbslam3/","series":[],"smallImg":"","tags":[{"title":"slam","url":"/tags/slam/"},{"title":"ros","url":"/tags/ros/"}],"timestamp":1680509685,"title":"Orb_slam3稠密建图"},{"authors":[],"categories":[],"content":"RealSense D455的标定 imu自校准 intel发布的d455深度相机自带的有一个imu，在对相机进行标定之前首先要校准一下imu\n校准操作见官方文档，官方提供有python脚本用于自动化校准，pdf中对具体操作有明确表述。这里提到一点操作时的一个bug\n正常情况下执行校准程序输出应当类似于\n只要按照pdf中示例的方式摆放相机，当三轴角度都为true的时候就会自动收集数据，但是我最开始遇到了\n一直停留在Status.rotate状态，无论怎么转都无法稳定在collect中，然后在github的issue中发现了相似问题\n解决方法：\n修改rs-imu-calibration.py第100行与616行，范围改大一些到0.6就可以了\nself.max_norm = np.linalg.norm(np.array([0.6, 0.6, 0.6])) max_norm = np.linalg.norm(np.array([0.6, 0.6, 0.6])) 参考链接:https://github.com/IntelRealSense/librealsense/issues/10418#issuecomment-1103125414\n打开realsense-viewer可以发现校准后竖直摆放相机，重力加速度近似于9.8，而校准前显示为8.8，说明校准起到效果\n联合标定 出现了以下bug，需要sudo apt install libelf-dev\n/home/username/packages/realsense_imu_catkin_ws/src/code_utils/include/code_utils/backward.hpp:216:25: fatal error: elfutils/libdw.h: No such file or directory 216 | # include \u0026lt;elfutils/libdw.h\u0026gt; | ^~~~~~~~~~~~~~~~~~ 但是安装的时候apt报错\nSome packages could not be installed. This may mean that you have requested an impossible situation or if you are using the unstable distribution that some required packages have not yet been created or been moved out of Incoming. The following information may help to resolve the situation: The following packages have unmet dependencies. libdw-dev : Depends: libelf-dev but it is not going to be installed Depends: libdw1 (= 0.165-3ubuntu1.2) but 0.176-1.1build1 is to be installed E: Unable to correct problems, you have held broken packages. 下载最新的安装包手动安装\nhttp://archive.ubuntu.com/ubuntu/pool/main/e/elfutils/libelf-dev_0.176-1.1build1_amd64.deb\nhttp://archive.ubuntu.com/ubuntu/pool/main/e/elfutils/libdw-dev_0.176-1.1build1_amd64.deb\n/home/username/packages/realsense_imu_catkin_ws/src/code_utils/src/sumpixel_test.cpp:2:10: fatal error: backward.hpp: No such file or directory 2 | #include \u0026#34;backward.hpp\u0026#34; | ^~~~~~~~~~~~~~ compilation terminated. 修改sumpixel_test.cpp第2行为`#include \u0026ldquo;code_utils/backward.hpp\u0026rdquo;\nerror: ‘integer_sequence’ is not a member of ‘std’ 75 | std::integer_sequence\u0026lt;T, N, Ns...\u0026gt; 修改CMakests.txt\nset(CMAKE_CXX_STANDARD 14) /home/username/packages/realsense_imu_catkin_ws/src/code_utils/src/mat_io_test.cpp:33:47: error: ‘CV_LOAD_IMAGE_UNCHANGED’ was not declared in this scope 33 | Mat img1 = imread( \u0026#34;/home/gao/IMG_1.png\u0026#34;, CV_LOAD_IMAGE_UNCHANGED ); | ^~~~~~~~~~~~~~~~~~~~~~~ opencv版本宏定义问题，改相关代码为新版的就行\nhttps://blog.csdn.net/weixin_44675820/article/details/124796674\nhttps://www.cnblogs.com/zzzsj/p/15699524.html\n/home/tioeare/packages/realsense_imu_catkin_ws/src/imu_utils/src/imu_an.cpp:68:19: error: aggregate ‘std::ofstream out_t’ has incomplete type and cannot be defined 修改imu_an.cpp，添加#include \u0026lt;fstream\u0026gt;\nErrors \u0026lt;\u0026lt; aslam_time:make /home/tioeare/packages/kalibr_workspace/logs/aslam_time/build.make.000.log /home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_cv/aslam_time/src/time.cpp:84:25: error: ISO C++17 does not allow dynamic exception specifications 84 | throw (NoHighPerformanceTimersException) | ^~~~~ make[2]: *** [CMakeFiles/aslam_time.dir/build.make:76: CMakeFiles/aslam_time.dir/src/time.cpp.o] Error 1 make[1]: *** [CMakeFiles/Makefile2:295: CMakeFiles/aslam_time.dir/all] Error 2 make: *** [Makefile:146: all] Error 2 把代码里这一行注释掉，很奇怪，这种用法应该C++11就禁止了，github上也没人提\ngyr x numData 240384 gyr x start_t 1.6799e+09 gyr x end_t 1.6799e+09 gyr x dt -------------600.003 s -------------10 min -------------0.166667 h gyr x freq 400.636 gyr x period 0.00249603 gyr y numData 240384 gyr y start_t 1.6799e+09 gyr y end_t 1.6799e+09 gyr y dt -------------600.003 s -------------10 min -------------0.166667 h gyr y freq 400.636 gyr y period 0.00249603 gyr z numData 240384 gyr z start_t 1.6799e+09 gyr z end_t 1.6799e+09 gyr z dt -------------600.003 s -------------10 min -------------0.166667 h gyr z freq 400.636 gyr z period 0.00249603 Gyro X C -2.40087 59.1038 -17.8144 3.38392 -0.0618206 Bias Instability 3.00267e-05 rad/s Bias Instability 5.92447e-05 rad/s, at 26.0036 s White Noise 16.8009 rad/s White Noise 0.00479981 rad/s bias 0.0580428 degree/s ------------------- Gyro y C -3.7193 89.2566 -24.0407 4.84558 -0.260302 Bias Instability 2.69712e-06 rad/s Bias Instability 9.61604e-05 rad/s, at 22 s White Noise 26.42 rad/s White Noise 0.00733927 rad/s bias -0.304682 degree/s ------------------- Gyro z C -1.0341 30.2021 -13.4864 3.95908 -0.244715 Bias Instability 4.5525e-06 rad/s Bias Instability 1.78885e-05 rad/s, at 56.7372 s White Noise 7.71113 rad/s White Noise 0.00214285 rad/s bias -0.095305 degree/s ------------------- ============================================== ============================================== acc x numData 240384 acc x start_t 1.6799e+09 acc x end_t 1.6799e+09 acc x dt -------------600.003 s -------------10 min -------------0.166667 h acc x freq 400.636 acc x period 0.00249603 acc y numData 240384 acc y start_t 1.6799e+09 acc y end_t 1.6799e+09 acc y dt -------------600.003 s -------------10 min -------------0.166667 h acc y freq 400.636 acc y period 0.00249603 acc z numData 240384 acc z start_t 1.6799e+09 acc z end_t 1.6799e+09 acc z dt -------------600.003 s -------------10 min -------------0.166667 h acc z freq 400.636 acc z period 0.00249603 acc X C -3.7565e-05 0.00153459 -0.000440891 5.14013e-05 1.36731e-05 Bias Instability 0.000415133 m/s^2 White Noise 0.0229567 m/s^2 ------------------- acc y C -0.000153981 0.00273578 0.000838508 -0.000456266 4.36107e-05 Bias Instability 0.000674808 m/s^2 White Noise 0.0392674 m/s^2 ------------------- acc z C -4.04458e-05 0.00242156 -0.00204122 0.000559034 -3.01212e-05 Bias Instability 0.000795269 m/s^2 White Noise 0.0348949 m/s^2 ------------------- [imu_an-1] process has finished cleanly Traceback (most recent call last): File \u0026#34;build/kalibr/atomic_configure/kalibr_calibrate_cameras\u0026#34;, line 15, in \u0026lt;module\u0026gt; exec(compile(fh.read(), python_script, \u0026#39;exec\u0026#39;), context) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_calibrate_cameras\u0026#34;, line 465, in \u0026lt;module\u0026gt; main() File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_calibrate_cameras\u0026#34;, line 175, in main cam = kcc.CameraGeometry(cameraModel, targetConfig, dataset, verbose=(parsed.verbose or parsed.showextraction)) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_camera_calibration/CameraCalibrator.py\u0026#34;, line 48, in __init__ self.ctarget = TargetDetector(targetConfig, self.geometry, showCorners=verbose) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_camera_calibration/CameraCalibrator.py\u0026#34;, line 81, in __init__ targetParams = targetConfig.getTargetParams() File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 187, in func return f(*args, **kwargs) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 620, in getTargetParams errList.append(\u0026#34;invalid tagSpacing (float)\u0026#34;) NameError: name \u0026#39;errList\u0026#39; is not defined 相应位置的文件中errList全部换成途中的print语句\nTraceback (most recent call last): File \u0026#34;build/kalibr/atomic_configure/kalibr_calibrate_imu_camera\u0026#34;, line 15, in \u0026lt;module\u0026gt; exec(compile(fh.read(), python_script, \u0026#39;exec\u0026#39;), context) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_calibrate_imu_camera\u0026#34;, line 247, in \u0026lt;module\u0026gt; main() File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_calibrate_imu_camera\u0026#34;, line 173, in main chain.printDetails() File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 765, in printDetails camConfig.printDetails(dest) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 387, in printDetails dist_model, dist_coeff = self.getDistortion() File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 187, in func return f(*args, **kwargs) File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 355, in getDistortion self.checkDistortion(self.data[\u0026#34;distortion_model\u0026#34;], File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 350, in checkDistortion self.raiseError(\u0026#34;distortion model \u0026#39;{}\u0026#39; requires {} coefficients; {} given\u0026#34;.format( File \u0026#34;/home/tioeare/packages/kalibr_workspace/src/kalibr/aslam_offline_calibration/kalibr/python/kalibr_common/ConfigReader.py\u0026#34;, line 235, in raiseError raise RuntimeError( \u0026#34;{0}{1}\u0026#34;.format(header, message) ) RuntimeError: [CameraConfig Reader]: distortion model \u0026#39;radtan\u0026#39; requires 4 coefficients; 1 given chain.yaml加逗号\npcl::transformPointCloud(*cloud_voxel_tem, *cloud1, T.matrix()); //报错 Signal: SIGSEGV (Segmentation fault) [pcl::VoxelGrid::applyFilter] Leaf size is too small for the input dataset. Integer indices would overflow. Assignment with new_width equal to 0,setting width to size of the cloud and height to 1 ","date":"March 26, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/daily/2023.03.26/","series":[],"smallImg":"","tags":[],"timestamp":1679778165,"title":"D455标定与pcl点云库的学习"},{"authors":[],"categories":[],"content":"周一至周三 由于交付时间有点紧，生产的同事主要在做交付到西藏的大型无人机，然后我就去帮忙装配小无人机\n周三到周五 继续我的算法部分，这两天先跑通了orb-slam3，ORB-SLAM是西班牙 Zaragoza 大学的 Raúl Mur-Arta 编写的视觉 SLAM 系统。 它是一个完整的 SLAM 系统，包括视觉里程计、跟踪、回环检测，是一种完全基于稀疏特征点的单目 SLAM 系统，同时还有单目、双目、RGBD 相机的接口。核心是使用 ORB (Orinted FAST and BRIEF) 作为整个视觉 SLAM 中的核心特征。\n这周首先在我的电脑上跑通了20年发布的orb-slam3。\n效果如下:\n图中左侧窗口为构建的稀疏地图，蓝色表示相机位姿，散点为提取的orb特征点，右侧窗口为双目红外相机实时拍摄到的图像，绿色点为图像中的检测到的特征点\n运行步骤 主要运行环境为ubuntu20.04系统，ROS版本为noetic，OpenCV版本4.5.5，eigen版本3.4.0\n在运行过程中主要官方文档，其中主要遇到了两个bug，如下\nopencv版本问题 在运行ROS的实例中，由于安装时ROS noetic自带的有4.2.0的OpenCV，cv_bridge也是4.2的，导致会出现如下错误\n\u0026#39;cv::Exception\u0026#39; what(): OpenCV(4.5.5) /home/ubuntu/Downloads/opencv-4.5.5/opencv/modules/core/src/matrix.cpp:250: error: (-215:Assertion failed) s \u0026gt;= 0 in function \u0026#39;setSize\u0026#39;` \u0026#39;cv::Exception\u0026#39; what(): OpenCV(4.5.5) /home/ubuntu/Downloads/opencv-4.5.5/opencv/modules/core/src/alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 87219830269440 bytes in function \u0026#39;OutOfMemoryError 需要自己再下载一个cv_bridge的源码，然后编译出本机4.5.5版本的cv_bridge，随后在orbslam3的CMakeLists.txt中修改如下\nset(cv_bridge_DIR \u0026#34;/home/username/path_to_cvbridge_build_ws/devel/share/cv_bridge/cmake\u0026#34;) set(OpenCV_DIR /usr/local/share/opencv4) LIST(APPEND CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake_modules) find_package(OpenCV 4.5.5 REQUIRED) if(NOT OpenCV_FOUND) message(FATAL_ERROR \u0026#34;OpenCV \u0026gt; 4.4 not found.\u0026#34;) endif() MESSAGE(\u0026#34;OPENCV VERSION:\u0026#34;) MESSAGE(${OpenCV_VERSION}) c++版本问题 原版默认的c++为c++11版本，但实操发现编译无法通过，需要修改为c++14，修改如下\n# Check C++11 or C++0x support include(CheckCXXCompilerFlag) CHECK_CXX_COMPILER_FLAG(\u0026#34;-std=c++14\u0026#34; COMPILER_SUPPORTS_CXX11) CHECK_CXX_COMPILER_FLAG(\u0026#34;-std=c++0x\u0026#34; COMPILER_SUPPORTS_CXX0X) if(COMPILER_SUPPORTS_CXX14) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} -std=c++14\u0026#34;) add_definitions(-DCOMPILEDWITHC11) message(STATUS \u0026#34;Using flag -std=c++14.\u0026#34;) elseif(COMPILER_SUPPORTS_CXX0X) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} -std=c++0x\u0026#34;) add_definitions(-DCOMPILEDWITHC0X) message(STATUS \u0026#34;Using flag -std=c++0x.\u0026#34;) else() message(FATAL_ERROR \u0026#34;The compiler ${CMAKE_CXX_COMPILER} has no C++11 support. Please use a different C++ compiler.\u0026#34;) endif() #添加c++14标准 eigen3 CMake Error at CMakeLists.txt:44 (find_package): Found package configuration file: /usr/local/lib/cmake/Pangolin/PangolinConfig.cmake but it set Pangolin_FOUND to FALSE so package \u0026#34;Pangolin\u0026#34; is considered to be NOT FOUND. Reason given by package: Pangolin could not be found because dependency Eigen3 could not be found. cmakelists.txt中eigen部分换为\nFIND_PACKAGE(Eigen3 REQUIRED NO_MODULE) intel realsense d455相关设置 需要将红外相机的红外发射器关掉，否则会极大影响特征的提取\nstereo_module emitter_enabled 需要将lanuch文件中这两项设为false\n下一周计划 目前的算法中只能建立起稀疏地图用于自身的定位，下一步需要借助pcl库和深度相机建立起稠密点云地图来实现避障和导航功能\n","date":"March 26, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/daily/2023.03.23/","series":[],"smallImg":"","tags":[],"timestamp":1679767365,"title":"装配并学习点云库"},{"authors":[],"categories":[{"title":"OpenCV","url":"/categories/opencv/"}],"content":"OpenCV识别停机坪 整体算法目标为能够在真机通过intel D435i深度相机与下视的工业相机完成无人机对周边环境的感知, 并建立起稠密地图. 计划目标为先在方针环境中进行算法编写与验证. 然后再部署到真机中进行调整和测试. 本周的主要完成的在仿真环境中对停机坪的识别, 并添加ros支持\n停机坪示意图如下\n停机坪识别概述 工程目录 identifyApron ├── bug.md\t#记录程序编写过程中遇到的bug及解决方法 ├── CMakeLists.txt\t#工程配置文件 ├── include\t#存放头文件 │ ├── getImage.h │ └── identify.h ├── media\t#存放测试或程序运行用到的图像文件 │ ├── E.jpg │ ├── image.png │ ├── N.jpg │ ├── S.jpg │ └── W.jpg ├── package.xml\t#ros依赖项配置 └── src\t#存放源文件 ├── getImage.cpp ├── identify.cpp └── main.cpp 本项目主要有getImage和identify两个文件, 其中getimage用于ros的订阅和发布图像节点. identify用于识别停机坪\n配置文件 CMakeLists.txt cmake中主要需要包含有eigen和opencv这两个库, 可以选用ros自带的opencv4.2然后通过ros安装eigen, 在这里我选用了之前本地安装过的eigen3.4和opencv4.5.5版本\n为能够使用本地版本, 需要在CMakeLists.txt中添加以下配置\nset(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} -std=c++14\u0026#34;) set(cv_bridge_DIR \u0026#34;/home/${your_username}/cvbridge_build_ws/devel/share/cv_bridge/cmake\u0026#34;) set(OpenCV_DIR /usr/local/share/opencv4) include_directories(${OpenCV_INCLUDE_DIRS}) include_directories(/home/${your_path_to_eigen}/eigen-3.4.0/Eigen) 首先指定C++版本为c++14, 以免某些特性无法通过编译. 其次需要自己手动重新编译一份cv_bridge, 原因是由于ros自带的cv_bridge匹配的是4.2版本, 直接用的话不和我电脑上的opencv适配, 会报类似于memory out of range的错误. 然后指定cv_bridge_dir为你重新编译的路径, 同时也需要手动指定opencv路径为你的安装路径\npackage.xml ros包依赖中需要以下依赖\n\u0026lt;buildtool_depend\u0026gt;catkin\u0026lt;/buildtool_depend\u0026gt; \u0026lt;build_depend\u0026gt;cv_bridge\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;image_transport\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;roscpp\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;sensor_msgs\u0026lt;/build_depend\u0026gt; \u0026lt;build_depend\u0026gt;std_msgs\u0026lt;/build_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;cv_bridge\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;image_transport\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;roscpp\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;sensor_msgs\u0026lt;/build_export_depend\u0026gt; \u0026lt;build_export_depend\u0026gt;std_msgs\u0026lt;/build_export_depend\u0026gt; \u0026lt;exec_depend\u0026gt;cv_bridge\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;image_transport\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;roscpp\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;sensor_msgs\u0026lt;/exec_depend\u0026gt; \u0026lt;exec_depend\u0026gt;std_msgs\u0026lt;/exec_depend\u0026gt; 主要用于订阅和发布图像传输的节点, 自定义发布数据\n算法实现 ros部分 ros部分主要就是订阅和发布图像, 下面根据代码进行解释\ngetImage.h //定义一个转换的类 class ImageDetector { private: ros::NodeHandle nh_; //定义ROS节点句柄 image_transport::ImageTransport it_; //定义一个image_transport实例,用来发布和订阅ROS系统的图像 image_transport::Subscriber image_sub_; //定义ROS图象接收器 ,订阅主题的变量 image_transport::Publisher image_pub_; //定义ROS图象发布器 ,发布主题的变量 identifyApron iden; public: ImageDetector(); ~ImageDetector() {} void convert_callback(const sensor_msgs::ImageConstPtr \u0026amp;msg); //回调函数包含识别和发布识别后的图片 void advertiseDone(); }; 首先头文件中定义一个用于将opencv的Mat类型的图像转化为ros中的数据格式, 在后面的源文件中有相关介绍, 头文件主要定义了用到的变量\n定义了ros相关节点句柄和用于识别的identifyApron对象, 两个函数分别用于接受图像并转化和发布图像. 其中接受到图像后在convert_callback函数中调用identifyApron的相关接口进行识别\ngetImage.cpp 源文件定义了类的构造函数和其他两个函数的写法\nImageDetector::ImageDetector() : it_(nh_) //构造函数 { // /iris_0/camera/image_raw image_sub_ = it_.subscribe(\u0026#34;/iris_0/stereo_camera/left/image_raw\u0026#34;, 1, \u0026amp;ImageDetector::convert_callback, this); //定义图象接受器 image_pub_ = it_.advertise(\u0026#34;/image_detector/output_video\u0026#34;, 1); //定义图象发布器 } 构造函数接受头文件中定义的句柄并定义订阅和发布使用的节点\nvoid ImageDetector::convert_callback(const sensor_msgs::ImageConstPtr \u0026amp;msg) { cv_bridge::CvImagePtr cv_ptr; try { cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8); } catch (cv_bridge::Exception \u0026amp;e) { ROS_ERROR(\u0026#34;cv_bridge exception: %s\u0026#34;, e.what()); return; } iden.detector(cv_ptr-\u0026gt;image); advertiseDone(); } void ImageDetector::advertiseDone() { sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), \u0026#34;bgr8\u0026#34;, iden.origin).toImageMsg(); image_pub_.publish(msg); } 转化函数调用cv_bridge对Mat类型进行转化, 然后调用identifyApron中的detector()进行图像的识别, 最后调用发布识别后的图像的函数\n识别部分 接下来是工程最主要的识别部分\nidentiyf.h class identifyApron { public: cv::Mat origin; void detector(const cv::Mat \u0026amp;frame); identifyApron(); private: cv::Scalar scalarLow = cv::Scalar(26, 43, 100); //hsv分离最小阈值 cv::Scalar scalarHigh = cv::Scalar(35, 255, 255); //hsv最大阈值 std::vector\u0026lt;std::vector\u0026lt;cv::Point\u0026gt;\u0026gt; allContours; //所有轮廓 std::vector\u0026lt;cv::Vec4i\u0026gt; hiera; //轮廓的层级关系 int mediaB = 3; //中值滤波窗口 int openEle = 2; //开运算尺寸 int closeEle = 3; //闭运算尺寸 int roughDilate = 52; std::vector\u0026lt;cv::Vec3f\u0026gt; circles; //霍夫圆的轮廓点 std::vector\u0026lt;cv::RotatedRect\u0026gt; alRect; //所有旋转矩形 std::vector\u0026lt;cv::RotatedRect\u0026gt; charRect; //字母的旋转矩形 cv::Point2f charRectPts[4]; std::vector\u0026lt;std::pair\u0026lt;std::string , int\u0026gt;\u0026gt; direction = {{\u0026#34;E\u0026#34;, 0}, //东西南北 {\u0026#34;W\u0026#34;, 1}, {\u0026#34;S\u0026#34;, 2}, {\u0026#34;N\u0026#34;, 3}}; std::vector\u0026lt;cv::Mat\u0026gt; chars; std::vector\u0026lt;cv::Mat\u0026gt; template_pictures; std::vector\u0026lt;Eigen::Matrix\u0026lt;int, 50, 50\u0026gt;\u0026gt; char_matrix; // int mat_size = 50; int matchMat(cv::Mat src); }; 头文件主要定义了函数接口和识别过程中用到的参数\nmatchMat()函数主要用于模板匹配, 为了识别出停机坪四周表示方向的字母\nidentify.cpp 识别文件中首先定义了几个后续用到的函数\nvoid restore_Rect(cv::Point2f pts[4], cv::Point2f points[4]); //还原外接矩形 float dis(cv::Point2f pts1, cv::Point2f pts2); //计算两点距离 void drawRect(cv::RotatedRect rect, const cv::Mat \u0026amp;image, const cv::Scalar scal); //画出旋转矩形 cv::Mat getRectROI(cv::RotatedRect rect, const cv::Mat \u0026amp;bilfilter); //得到旋转矩形的roi 这几个函数的用途如注释中写的, 用于还原外接矩形四个点的顺序, 计算像素点之间的距离等\n主要的detector()函数首先将获取到的图像进行预处理\norigin = frame.clone(); cv::Mat hsv, filter, bilfilter, masked, cannied, round; cv::cvtColor(origin, hsv, cv::COLOR_BGR2HSV); cv::inRange(hsv, scalarLow, scalarHigh, masked); cv::medianBlur(masked, filter, mediaB); cv::bilateralFilter(filter, bilfilter, 5, 10, 3); cv::Mat struElement = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(openEle, openEle), cv::Point(-1, -1)); cv::morphologyEx(bilfilter, bilfilter, cv::MORPH_OPEN, struElement); struElement = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(closeEle, closeEle), cv::Point(-1, -1)); cv::morphologyEx(bilfilter, bilfilter, cv::MORPH_CLOSE, struElement); 先复制一份图像以免处理图像时受到其他意外影响本函数的运行\n然后将bgr格式的图像转化到hsv色彩空间中, 这是由于bgr表示的图像像素点会受到光照影响从而改变颜色范围, 而hsv将图像表示为色相, 饱和度, 亮度三部分. 将亮度和色相分割开, 有效避免了阴影与图像颜色的影响\n可以看到阈值化后已经隔除了红色墙壁和绿色部分对识别的影响, 但是图像中仔细观察会发现还有白色部分里有一些小黑点, 会影响到后续的识别. 因此需要再进行膨胀, 腐蚀, 滤波等处理\ncv::medianBlur(masked, filter, mediaB); cv::bilateralFilter(filter, bilfilter, 5, 10, 3); cv::Mat struElement = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(openEle, openEle), cv::Point(-1, -1)); cv::morphologyEx(bilfilter, bilfilter, cv::MORPH_OPEN, struElement); struElement = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(closeEle, closeEle), cv::Point(-1, -1)); cv::morphologyEx(bilfilter, bilfilter, cv::MORPH_CLOSE, struElement); cv::Canny(round, cannied, 10, 250, 5); 首先中值滤波取出可能存在的椒盐噪点, 然后双边滤波和闭运算, 处理完成后再进行边缘检测用于霍夫圆监测\n边缘检测效果如下\n在上上一步中获取到的图像在闭运算后可以用于检测字母, 其主要思想为\n查找图像中的所有轮廓并列出轮廓的层级关系, 很明显需要检测的字母轮廓是没有子轮廓和父轮廓的, 但是停机坪的圆弧同样没有父子轮廓 但观察很容易可以看出字母的轮廓的最小外接矩形近似于正方形, 而圆弧的最小外接矩形为长宽差异很大的长方形. 因此可以先求出外接矩形的长宽比然后筛选得出字母的轮廓 得到轮廓后需要进行识别, 这里识别字母可以用神经网络的方法进行训练(如knn, lenet等). 但是停机坪字母固定并且只有四个, 没有必要训练那么多, 因此我采用了类似模板匹配的方法 首先制作一个字母灰度图的模板, 然后将2中获得的最小外接矩形旋转到正方向, 进行缩放为模板大小, 然后再循环90, 190, 270三种度数与模板做矩阵减法并求和, 此处用到了eigen将Mat转化为Matrix然后加快求解速度 ///检测字母 cv::findContours(bilfilter, allContours, hiera, cv::RETR_TREE, cv::CHAIN_APPROX_SIMPLE); //查找轮廓 // cv::drawContours(origin, allContours, -1, cv::Scalar(0, 0, 255), 1); int i = 0; for (const auto \u0026amp;itcHier: hiera) { if (itcHier[2] == -1 \u0026amp;\u0026amp; itcHier[3] == -1) { alRect.push_back(cv::minAreaRect(allContours[i])); }//没有子轮廓和父轮廓可能是目标圆弧或字母 ++i; } cv::Point2f pts[4], pts1[4]; i = 0; for (const auto \u0026amp;rect: alRect) { rect.points(pts); restore_Rect(pts, pts1); float width = dis(pts1[0], pts1[3]); float height = dis(pts1[0], pts1[1]); if ((width / height) \u0026gt; 0.8 \u0026amp;\u0026amp; (width / height) \u0026lt; 1.2) { ///调参, 去除圆弧 charRectPts[i].x = pts1[0].x; charRectPts[i].y = pts1[0].y; ++i; charRect.push_back(rect); drawRect(rect, origin, cv::Scalar(0, 255, 100)); chars.push_back(getRectROI(rect, bilfilter)); } } if(chars.size() != 0){ for (i = 0; i \u0026lt; chars.size(); i++) { int number = matchMat(chars[i]); //std::to_string(number) cv::putText(origin, direction[number].first, cv::Point((int) charRectPts[i].x, (int) charRectPts[i].y), cv::FONT_HERSHEY_SIMPLEX, 1.2, cv::Scalar(50, 100, 255), 2, cv::LINE_AA); } } 模板匹配实现如下\nint identifyApron::matchMat(cv::Mat src) { int angle = 0; if (false) { label: if (angle \u0026gt; 270) return 4; cv::Size dst_sz(src.cols, src.rows); cv::Point2f center(static_cast\u0026lt;float\u0026gt;(src.cols / 2.), static_cast\u0026lt;float\u0026gt;(src.rows / 2.)); angle += 90; cv::Mat rot_mat = cv::getRotationMatrix2D(center, angle, 1.0); cv::warpAffine(src, src, rot_mat, dst_sz, cv::INTER_LINEAR, cv::BORDER_REPLICATE); } int i = 0; for (const Eigen::Matrix\u0026lt;int, 50, 50\u0026gt; \u0026amp;matrix: char_matrix) { Eigen::Matrix\u0026lt;int, 50, 50\u0026gt; src_matrix; cv::cv2eigen(src, src_matrix); Eigen::ArrayXXi result = (matrix - src_matrix).array().abs(); long re = (long) result.sum(); if (re \u0026lt; 150000) { std::cout \u0026lt;\u0026lt; result; return i; } else if (i \u0026gt;= 3) { goto label; } ++i; } return 4; } 该函数接受待匹配字母, 返回头文件中定义的std::vector\u0026lt;std::pair\u0026lt;std::string , int\u0026gt;\u0026gt; direction, 用\u0026lt;字母, 数字\u0026gt;表示方向\n最终识别效果如下\n很好的识别出了四个方向的字母, 并拟合无人机的大概降落位置\n","date":"March 16, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/project/indentifyapron/","series":[],"smallImg":"","tags":[{"title":"c++","url":"/tags/c++/"},{"title":"OpenCV","url":"/tags/opencv/"}],"timestamp":1678954485,"title":"Gazebo仿真环境下停机坪的识别"},{"authors":[],"categories":[{"title":"服务器","url":"/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"}],"content":"Caddy v2, webhook, hugo搭建博客并配置bitwarden与v2ray 项目概述 版本说明 caddy 之前我的服务器(amd64, debain10)上一直用的是caddy 1.x版本, 目前官方为了推广2.0版本, 已经绝情的将v1版本的官方下载渠道, 插件安装脚本以及文档等全都删掉了🙃, 导致我最开始看的关于v1版搭建博客的教程全都用不了. 并且由于v1版已经发布很多年了, 而v2在2020年才发布. 网上绝大部分教程以及git, webhook相关的插件还都是v1版的, 对纯萌新来说挺难找到合适的教程来详细描述具体如何配置. 目前网上也有了很多大佬开源的包含有caddy v2, hugo等的docker镜像可以方便的一键搭建博客. 但不知道为什么在我的服务器上运行时似乎没有办法正常监听端口一样，总是接受不到相关的请求\n此外, caddy v2与v1相比配置文件改成了原生json格式, 之前v1版本的时候使用的配置文件是Caddyfile, 写起来即简便又易读. 现在换成json之后, 虽然还是可以用Caddyfile来写, 但是实际运行caddy似乎会先将Caddyfile转化成json然后再运行. 这就导致写在Caddyfile里的配置, 尤其是相关插件的配置很可能在转化的时候出现某些问题导致运行失败. 还有另外一个很重要的问题是, 大多v2版本的caddy插件是支持Caddyfile的写法的, 但有些插件就只有json格式的配置写法. 而前面也提到Caddyfile是可以被caddy转化为json格式的, 可以使用caddy adapt命令显式转化文件。所以即使部分插件在文档中只写了Caddyfile的写法, 你也可以先写这部分的配置写道Caddyfile里然后再转化为json继续配置\n总上所述, 最终选择了本地使用xcaddy构建带有vrongmeal/caddygit, WingLim/caddy-webhook , mholt/caddy-webdav这三个插件的caddy v2.6.4在本地运行json格式配置文件的方式完成服务器的路由配置\nbitwarden bitwarden是一个开源, 方便的密码管理器, 在网页(chrome, firefox, safair)/android/ios/windows/linux等全平台都有相关客户端(网页插件), 支持部署到自己的服务器构建自己专属的密码库，有效的提高了安全性. 之前caddy配置bitwarden的时候, 可以使用官方的docker镜像, 只用映射一下端口就可以一键部署. 现在caddy升级到v2之后好像对bitwarden还不适配, 只能使用最新版本的vaultwarden(bitwarden的一个第三方版本)来部署\nhugo hugo是一个方便的从md文档构建静态网页的工具, 非常符合我目前的需求. 但是注意很多网站的主题要求hugo是extend版(hugo官方也推荐使用extend版), 同时由于主题模板的更新, 很多主题对hugo的版本要求会比较新. 但apt安装的是0.55版的, 不符合我目前主题的要求. 官方还提供了snap安装, 但我在服务器上装的时候出现了好像说系统不匹配的错误😔, 看github的issue上有人提过类似的, 下面有个回答说他已经修复了, 可是我的这个还是不行. 所以最后采用了在github的release里下载extend版的deb安装包，然后再在本地dpkg -i安装\n格式说明 本文假定你已经有一个属于自己的服务器和域名, 并且已经把dns解析好了, 国内服务器的备案也不在本文讨论范围内. 本文仅包含caddy的安装配置, 配置文件caddyfile.json的编写, hugo的安装配置和vaultwarden docker镜像的配置\n本文接下来在配置文件中出现的必需需要读者本人根据自己服务器等进行修改的地方会使用类似@1{path}的方式代指，例如你本地的某个文件夹地址为/usr/local/bin, 在配置文件中会以@1{path}代指这个地址(数字指的是这个文件中出现的第几个, 如果两个地方用一个的话就是一个地址). 同理@1{domain}代指文件中第一个出现的需要你修改成自己的域名www.your-domain.com\n安装 caddy caddy从官方途径apt安装的是没有插件的, 没插件没办法完成接下来的部署, 但是从apt安装的会配套相关的systemd服务, 方便后续管理运行重启等. 因此先apt安装没有插件的caddy然后再通过xcaddy来编译带有插件的caddy, 编译完成后替换掉之前apt安装的caddy\napt安装caddy apt install -y debian-keyring debian-archive-keyring apt-transport-https curl -1sLf \u0026#39;https://dl.cloudsmith.io/public/caddy/stable/gpg.key\u0026#39; | gpg --dearmor -o /usr/share/keyrings/caddy-stable-archive-keyring.gpg curl -1sLf \u0026#39;https://dl.cloudsmith.io/public/caddy/stable/debian.deb.txt\u0026#39; | tee /etc/apt/sources.list.d/caddy-stable.list apt update apt install caddy go环境 由于xcaddy是由go语言编译安装的, 所以需要先配置环境\nwget https://go.dev/dl/go1.20.7.linux-amd64.tar.gz rm -rf /usr/local/go \u0026amp;\u0026amp; tar -C /usr/local -xzf go1.20.7.linux-amd64.tar.gz \u0026amp;\u0026amp; rm go1.20.7.linux-amd64.tar.gz vim ~/.bashrc #写入文件 export PATH=$PATH:/usr/local/go/bin #保存文件 source ~/.bashrc xcaddy 安装xcaddy\ngo install github.com/caddyserver/xcaddy/cmd/xcaddy@latest xcaddy编译带插件的caddy\nexport version=$(curl -s \u0026#34;https://api.github.com/repos/caddyserver/caddy/releases/latest\u0026#34; | grep tag_name | cut -f4 -d \u0026#34;\\\u0026#34;\u0026#34;) go/bin/xcaddy build ${version} \\ --with github.com/WingLim/caddy-webhook \\ --with github.com/mholt/caddy-webdav \\ --with github.com/vrongmeal/caddygit 之后会编译出带有这三个插件的caddy到当前目录, 如果你需要其他插件的话, 后面--with接着跟插件的github仓库地址就行了\n编译完成后使用whereis caddy查看之前apt安装的caddy安装到哪里了, 再用mv把刚编译的带插件的替换掉apt安装的\nmv caddy /usr/bin/caddy hugo export version=$(curl -s \u0026#34;https://api.github.com/repos/gohugoio/hugo/releases/latest\u0026#34; | grep tag_name | cut -f4 -d \u0026#34;\\\u0026#34;\u0026#34; | grep -oE \u0026#39;([0-9]+\\.?)+\u0026#39;) wget https://github.com/gohugoio/hugo/releases/download/v${version}/hugo_extended_${version}_linux-amd64.deb dpkg -i hugo_extended_${version}_linux-amd64.deb vaultwarden 需要先安装docker, 安装好后再\napt-get install ca-certificates curl gnupg install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=\u0026#34;$(dpkg --print-architecture)\u0026#34; signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null apt-get update apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker run -d --name vaultwarden \\ -e SIGNUPS_ALLOWED=true \\ -e WEBSOCKET_ENABLED=true \\ -e LOG_FILE=/data/bitwarden.log \\ -p @1{vault_port}:80 \\ -p @2{vault_port}:3012 \\ -v /vw-data/:/data/ \\ vaultwarden/server:1.27.0 此处的@1{vault-port}和@2{vault-port}需要替换掉成之后你在caddy的配置文件中反代的端口, 因此建议配置好其他之后再安装这个\n配置 变量解释如下，需要将下面所有配置文件中出现的地方替换成你自己的\n@1{vault_port}:80\t--caddy反代的bitwarden端口，例如8080：80 @2{vault_port}:3012\t--caddy反代的bitwarden端口，例如3012：3012 @3{v2ray_port}\t--caddy反代的v2ray端口，例如2233 @1{cer_path}\t--bitwarden网站的证书cer部分，例如usr/local/etc/ssl/bitwarden.cer @1{key_path} --bitwarden网站的证书key部分, 例如usr/local/etc/ssl/bitwarden.key @2{cer_path}\t--blog网站的证书cer部分 @2{key_path}\t--blog网站的证书key部分 @3{v2ray_path}\t--v2ray的访问路径，以/开头，例如/vvv。但是不能和@4{v2ray_www_path}的最后一级相同，即如果@4{v2ray_www_path}是/var/www/v2ray的话，就不能是/v2ray @4{v2ray_www_path}\t--v2ray网站的存放路径，例如/var/www/v2ray @5{blog_path}\t--存放博客网站的地址, 例如/var/www/myblog @1{vault_domain}\t--bitwarden的域名 @2{blog_domain}\t--blog的域名 @3{v2ray_domain}\t--v2ray的伪装域名 @1{github_url}\t--存放博客文档的github仓库地址 @1{github_token}\t--有存放博客文档的github仓库访问权限的token @1{github_secret}\t--webhook的secret @1{v2ray_uuid}\t--v2ray配置 @1{your_email}\t--注册域名时使用的邮箱 @1{cloudflare_key}\t--cloudflare的key caddy caddy.service 首先需要修改以下caddy.service配置相关服务\nvim /etc/systemd/system/caddy.service # caddy.service # # For using Caddy with a config file. # # Make sure the ExecStart and ExecReload commands are correct # for your installation. # # See https://caddyserver.com/docs/install for instructions. # # WARNING: This service does not use the --resume flag, so if you # use the API to make changes, they will be overwritten by the # Caddyfile next time the service is restarted. If you intend to # use Caddy\u0026#39;s API to configure it, add the --resume flag to the # `caddy run` command or use the caddy-api.service file instead. [Unit] Description=Caddy Documentation=https://caddyserver.com/docs/ After=network.target network-online.target Requires=network-online.target [Service] Type=notify User=caddy Group=caddy ExecStart=/usr/bin/caddy run --environ --config /etc/caddy/caddyfile.json ExecReload=/usr/bin/caddy reload --config /etc/caddy/caddyfile.json --force TimeoutStopSec=5s LimitNOFILE=1048576 LimitNPROC=512 PrivateTmp=true ProtectSystem=full AmbientCapabilities=CAP_NET_BIND_SERVICE [Install] WantedBy=multi-user.target chown root:root /etc/systemd/system/caddy.service chmod 644 /etc/systemd/system/caddy.service systemctl daemon-reload chown root:root /usr/bin/caddy chmod 755 /usr/bin/caddy setcap \u0026#39;cap_net_bind_service=+ep\u0026#39; /usr/bin/caddy groupadd -g 33 caddy useradd \\ -g caddy --no-user-group \\ --home-dir /var/caddy --no-create-home \\ --shell /usr/sbin/nologin \\ --system --uid 33 caddy mkdir /etc/caddy mkdir /var/www mkdir /var/caddy mkdir /etc/ssl/caddy touch /etc/caddy/caddyfile.json touch /var/log/caddy.log chown caddy:caddy /var/caddy chown -R root:caddy /etc/caddy chown -R root:caddy /etc/ssl/caddy chown -R root:caddy /var/www chmod 555 /var/caddy chmod 777 /var/www chmod 0770 /etc/ssl/caddy chown root:caddy /var/log/caddy.log chown root:root /etc/caddy/caddyfile.json chmod 770 /var/log/caddy.log chmod 644 /etc/caddy/caddyfile.json caddyfile.json 接着配置caddyfile.json\nvim /etc/caddy/caddyfile.json { \u0026#34;logging\u0026#34;: { \u0026#34;logs\u0026#34;: { \u0026#34;default\u0026#34;: { \u0026#34;exclude\u0026#34;: [ \u0026#34;http.log.access.log0\u0026#34;, \u0026#34;http.log.access.log1\u0026#34;, \u0026#34;http.log.access.log2\u0026#34; ] }, \u0026#34;log0\u0026#34;: { \u0026#34;writer\u0026#34;: { \u0026#34;filename\u0026#34;: \u0026#34;/var/log/caddy.log\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;file\u0026#34;, \u0026#34;roll_keep\u0026#34;: 10, \u0026#34;roll_size_mb\u0026#34;: 10 }, \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;include\u0026#34;: [ \u0026#34;http.log.access.log0\u0026#34; ] }, \u0026#34;log1\u0026#34;: { \u0026#34;writer\u0026#34;: { \u0026#34;filename\u0026#34;: \u0026#34;/var/log/caddy.log\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;include\u0026#34;: [ \u0026#34;http.log.access.log1\u0026#34; ] }, \u0026#34;log2\u0026#34;: { \u0026#34;writer\u0026#34;: { \u0026#34;filename\u0026#34;: \u0026#34;/var/log/caddy.log\u0026#34;, \u0026#34;output\u0026#34;: \u0026#34;file\u0026#34; }, \u0026#34;include\u0026#34;: [ \u0026#34;http.log.access.log2\u0026#34; ] } } }, \u0026#34;apps\u0026#34;: { \u0026#34;tls\u0026#34;: { \u0026#34;certificates\u0026#34;: { \u0026#34;load_files\u0026#34;: [ { \u0026#34;certificate\u0026#34;: \u0026#34;@1{cer_path}\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;@1{key_path}\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;cert0\u0026#34; ] }, { \u0026#34;certificate\u0026#34;: \u0026#34;@2{cer_path}\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;@2{cer_path}\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;cert1\u0026#34; ] } ] }, \u0026#34;automation\u0026#34;: { \u0026#34;policies\u0026#34;: [ { \u0026#34;subjects\u0026#34;: [ \u0026#34;@1{vault_domain}\u0026#34;, \u0026#34;@2{blog_domain}\u0026#34; ] }, { \u0026#34;subjects\u0026#34;: [ \u0026#34;@3{v2ray_domain}\u0026#34; ], \u0026#34;issuers\u0026#34;: [ { \u0026#34;email\u0026#34;: \u0026#34;@1{your_email}\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;acme\u0026#34; }, { \u0026#34;email\u0026#34;: \u0026#34;@1{your_email}\u0026#34;, \u0026#34;module\u0026#34;: \u0026#34;zerossl\u0026#34; } ] } ] } }, \u0026#34;http\u0026#34;: { \u0026#34;servers\u0026#34;: { \u0026#34;srv1\u0026#34;: { \u0026#34;listen\u0026#34;: [ \u0026#34;:80\u0026#34; ], \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: [ { \u0026#34;host\u0026#34;: [ \u0026#34;@1{vault_domain}\u0026#34; ] } ], \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;static_response\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;Location\u0026#34;: [ \u0026#34;https://@1{vault_domain}\u0026#34; ] }, \u0026#34;status_code\u0026#34;: 302 } ] } ] } ], \u0026#34;terminal\u0026#34;: true }, { \u0026#34;match\u0026#34;: [ { \u0026#34;host\u0026#34;: [ \u0026#34;@2{blog_domain}\u0026#34; ] } ], \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;static_response\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;Location\u0026#34;: [ \u0026#34;https://@2{blog_domain}\u0026#34; ] }, \u0026#34;status_code\u0026#34;: 302 } ] } ] } ], \u0026#34;terminal\u0026#34;: true }, { \u0026#34;match\u0026#34;: [ { \u0026#34;host\u0026#34;: [ \u0026#34;@3{v2ray_domain}\u0026#34; ] } ], \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;static_response\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;Location\u0026#34;: [ \u0026#34;https://@3{v2ray_domain}\u0026#34; ] }, \u0026#34;status_code\u0026#34;: 302 } ] } ] } ], \u0026#34;terminal\u0026#34;: true } ] }, \u0026#34;srv0\u0026#34;: { \u0026#34;listen\u0026#34;: [ \u0026#34;:443\u0026#34; ], \u0026#34;tls_connection_policies\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;sni\u0026#34;: [ \u0026#34;@2{blog_domain}\u0026#34; ] }, \u0026#34;certificate_selection\u0026#34;: { \u0026#34;any_tag\u0026#34;: [ \u0026#34;cert1\u0026#34; ] } }, { \u0026#34;match\u0026#34;: { \u0026#34;sni\u0026#34;: [ \u0026#34;@1{vault_domain}\u0026#34; ] }, \u0026#34;certificate_selection\u0026#34;: { \u0026#34;any_tag\u0026#34;: [ \u0026#34;cert0\u0026#34; ] } }, { \u0026#34;match\u0026#34;: { \u0026#34;sni\u0026#34;: [ \u0026#34;@3{v2ray_domain}\u0026#34; ] }, \u0026#34;cipher_suites\u0026#34;: [ \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\u0026#34; ], \u0026#34;protocol_min\u0026#34;: \u0026#34;tls1.2\u0026#34;, \u0026#34;protocol_max\u0026#34;: \u0026#34;tls1.3\u0026#34; }, {} ], \u0026#34;logs\u0026#34;: { \u0026#34;logger_names\u0026#34;: { \u0026#34;@3{v2ray_domain}\u0026#34;: \u0026#34;log0\u0026#34; }, \u0026#34;logger_names\u0026#34;: { \u0026#34;@1{vault_domain}\u0026#34;: \u0026#34;log1\u0026#34; }, \u0026#34;logger_names\u0026#34;: { \u0026#34;@2{blog_domain}\u0026#34;: \u0026#34;log2\u0026#34; }, \u0026#34;skip_hosts\u0026#34;: [ \u0026#34;@1{vault_domain}\u0026#34;, \u0026#34;@2{blog_domain}\u0026#34;, \u0026#34;@3{v2ray_domain}\u0026#34; ] }, \u0026#34;routes\u0026#34;: [ { \u0026#34;match\u0026#34;: [ { \u0026#34;host\u0026#34;: [ \u0026#34;@3{v2ray_domain}\u0026#34; ] } ], \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;vars\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;@4{v2ray_www_path}\u0026#34; }, { \u0026#34;encodings\u0026#34;: { \u0026#34;gzip\u0026#34;: {} }, \u0026#34;handler\u0026#34;: \u0026#34;encode\u0026#34;, \u0026#34;prefer\u0026#34;: [ \u0026#34;gzip\u0026#34; ] } ] }, { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;reverse_proxy\u0026#34;, \u0026#34;upstreams\u0026#34;: [ { \u0026#34;dial\u0026#34;: \u0026#34;localhost:@3{v2ray_port}\u0026#34; } ] } ], \u0026#34;match\u0026#34;: [ { \u0026#34;header\u0026#34;: { \u0026#34;Connection\u0026#34;: [ \u0026#34;*Upgrade*\u0026#34; ], \u0026#34;Upgrade\u0026#34;: [ \u0026#34;websocket\u0026#34; ] }, \u0026#34;path\u0026#34;: [ \u0026#34;@3{v2ray_path}\u0026#34; ] } ] }, { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;file_server\u0026#34;, \u0026#34;hide\u0026#34;: [ \u0026#34;/etc/caddy/Caddyfile\u0026#34; ] } ] } ] } ], \u0026#34;terminal\u0026#34;: true }, { \u0026#34;match\u0026#34;: [ { \u0026#34;host\u0026#34;: [ \u0026#34;@1{vault_domain}\u0026#34; ] } ], \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;encodings\u0026#34;: { \u0026#34;gzip\u0026#34;: {} }, \u0026#34;handler\u0026#34;: \u0026#34;encode\u0026#34;, \u0026#34;prefer\u0026#34;: [ \u0026#34;gzip\u0026#34; ] } ] }, { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;reverse_proxy\u0026#34;, \u0026#34;upstreams\u0026#34;: [ { \u0026#34;dial\u0026#34;: \u0026#34;localhost:@2{vault_port}\u0026#34; } ] } ], \u0026#34;match\u0026#34;: [ { \u0026#34;path\u0026#34;: [ \u0026#34;/notifications/hub\u0026#34; ] } ] }, { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;reverse_proxy\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;request\u0026#34;: { \u0026#34;set\u0026#34;: { \u0026#34;X-Real-Ip\u0026#34;: [ \u0026#34;localhost\u0026#34; ] } } }, \u0026#34;upstreams\u0026#34;: [ { \u0026#34;dial\u0026#34;: \u0026#34;localhost:@1{vault_port}\u0026#34; } ] } ] } ] } ], \u0026#34;terminal\u0026#34;: true }, { \u0026#34;match\u0026#34;: [ { \u0026#34;host\u0026#34;: [ \u0026#34;@5{blog_domain}\u0026#34; ] } ], \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;vars\u0026#34;, \u0026#34;root\u0026#34;: \u0026#34;@5{blog_path}/public\u0026#34; }, { \u0026#34;handler\u0026#34;: \u0026#34;file_server\u0026#34;, \u0026#34;hide\u0026#34;: [ \u0026#34;/etc/caddy/Caddyfile\u0026#34; ] } ] } ] } ], \u0026#34;terminal\u0026#34;: true, \u0026#34;match\u0026#34;: [ { \u0026#34;not\u0026#34;: [ { \u0026#34;path\u0026#34;: [ \u0026#34;/webhook\u0026#34; ] } ] } ] }, { \u0026#34;handle\u0026#34;: [ { \u0026#34;handler\u0026#34;: \u0026#34;subroute\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;handle\u0026#34;: [ { \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;handler\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;@5{blog_path}/content\u0026#34;, \u0026#34;repo\u0026#34;: \u0026#34;@1{github_url}\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;@1{github_token}\u0026#34;, \u0026#34;secret\u0026#34;: \u0026#34;@1{github_secret}\u0026#34;, \u0026#34;command\u0026#34;: [ \u0026#34;hugo\u0026#34;, \u0026#34;--destination\u0026#34;, \u0026#34;@5{blog_path}/public\u0026#34; ], \u0026#34;submodule\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;github\u0026#34; } ] } ] } ], \u0026#34;match\u0026#34;: [ { \u0026#34;path\u0026#34;: [ \u0026#34;/webhook\u0026#34; ] } ] } ] } } } } } v2ray https://github.com/zxcvos/Xray-script\nconfig.json mkdir -p /usr/local/etc/v2ray/\nmkdir -p /usr/local/etc/ssl/ecc/\nvim /usr/local/etc/v2ray/config.json\n{ \u0026#34;log\u0026#34;: { \u0026#34;access\u0026#34;: \u0026#34;/var/log/v2ray/access.log\u0026#34;, \u0026#34;error\u0026#34;: \u0026#34;/var/log/v2ray/error.log\u0026#34;, \u0026#34;loglevel\u0026#34;: \u0026#34;warning\u0026#34; }, \u0026#34;dns\u0026#34;: {}, \u0026#34;stats\u0026#34;: {}, \u0026#34;inbounds\u0026#34;: [ { \u0026#34;port\u0026#34;: @3{v2ray_port}, \u0026#34;listen\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;vmess\u0026#34;, \u0026#34;settings\u0026#34;: { \u0026#34;clients\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;@1{v2ray_uuid}\u0026#34;, \u0026#34;level\u0026#34;: 0, \u0026#34;email\u0026#34;: \u0026#34;@1{your_email}\u0026#34; } ], \u0026#34;decryption\u0026#34;: \u0026#34;none\u0026#34; }, \u0026#34;streamSettings\u0026#34;: { \u0026#34;network\u0026#34;: \u0026#34;ws\u0026#34;, \u0026#34;security\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;wsSettings\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;@3{v2ray_path}\u0026#34; } } } ], \u0026#34;outbounds\u0026#34;: [ { \u0026#34;tag\u0026#34;: \u0026#34;direct\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;freedom\u0026#34;, \u0026#34;settings\u0026#34;: {} }, { \u0026#34;tag\u0026#34;: \u0026#34;blocked\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;blackhole\u0026#34;, \u0026#34;settings\u0026#34;: {} } ], \u0026#34;routing\u0026#34;: { \u0026#34;domainStrategy\u0026#34;: \u0026#34;IPIfNonMatch\u0026#34;, \u0026#34;rules\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;field\u0026#34;, \u0026#34;domain\u0026#34;: [ \u0026#34;googleapis.cn\u0026#34;, \u0026#34;goofle.cn\u0026#34; ], \u0026#34;outboundTag\u0026#34;: \u0026#34;direct\u0026#34; }, { \u0026#34;outboundTag\u0026#34;: \u0026#34;blocked\u0026#34;, \u0026#34;ip\u0026#34;: [ \u0026#34;geoip:private\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;field\u0026#34; } ] }, \u0026#34;policy\u0026#34;: {}, \u0026#34;reverse\u0026#34;: {}, \u0026#34;transport\u0026#34;: {} } hugo hugo new site @5{blog_path} 会在@5{blog_path}处建立网站的目录\nvaultwarden 编写脚本监视git活动并更新网页 按理来说是不需要这一步的，应该是caddy接收到github发来的webhook然后就启动相关插件把更新的仓库pull下来, 然后就调用hugo --destination @5{blog_path}/public命令更新网页了\n但实际上发现这个命令居然是在/根目录下执行的, 即使在caddyfile.json里指定root * @5{blog_path} 了也不行.\n开始还想把命令换成执行其他的脚本, 然后在脚本中cd到路径再执行hugo生成网页. 但是好像也没法正常运行起来\n只能退而求其次, 单独编写一个inotify.sh监测@5{blog_path}/content/.git/index这个文件的变动情况然后再执行更新网页, 脚本如下(需要先安装inotify)\n#!/bin/sh filename=$@5{blog_path}/content/.git/index inotifywait -mrq --format \u0026#39;%e\u0026#39; --event create,delete,modify $filename | while read event do cd @5{blog_path} \u0026amp;\u0026amp; hugo done 之后执行nohup bash inotify.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1让脚本在后台运行就可以了\n为网站申请证书 (以cloudflare为例) 安装acme.sh\ncurl https://get.acme.sh | sh source ~/.bashrc export CF_Key=\u0026#34;@1{cloudflare_key}\u0026#34; export CF_Email=\u0026#34;@1{your_email}\u0026#34; acme.sh --register-account -m @1{your_email} 之后运行如下命令即可为blog网站申请证书\nacme.sh --issue --dns dns_cf -d @2{blog_domain} --keylength ec-256 #--debug 2 如果出错可以加这个参数看看哪里报错 如下命令可将证书安装在@2{cer_path}, @2{key_path}路径下\nacme.sh --installcert -d @2{blog_domain} --ecc --key-file @2{key_path} --fullchain-file @2{cer_path} --force vim /etc/sysctl.conf # max open files fs.file-max = 51200 # max read buffer net.core.rmem_max = 67108864 # max write buffer net.core.wmem_max = 67108864 # default read buffer net.core.rmem_default = 65536 # default write buffer net.core.wmem_default = 65536 # max processor input queue net.core.netdev_max_backlog = 4096 # max backlog net.core.somaxconn = 4096 # resist SYN flood attacks net.ipv4.tcp_syncookies = 1 # reuse timewait sockets when safe net.ipv4.tcp_tw_reuse = 1 # turn off fast timewait sockets recycling net.ipv4.tcp_tw_recycle = 0 # short FIN timeout net.ipv4.tcp_fin_timeout = 30 # short keepalive time net.ipv4.tcp_keepalive_time = 1200 # outbound port range net.ipv4.ip_local_port_range = 10000 65000 # max SYN backlog net.ipv4.tcp_max_syn_backlog = 4096 # max timewait sockets held by system simultaneously net.ipv4.tcp_max_tw_buckets = 5000 # TCP receive buffer net.ipv4.tcp_rmem = 4096 87380 67108864 # TCP write buffer net.ipv4.tcp_wmem = 4096 65536 67108864 # turn on path MTU discovery net.ipv4.tcp_mtu_probing = 1 # for high-latency network net.core.default_qdisc=fq net.ipv4.tcp_congestion_control = bbr sysctl -p apt install libcap2-bin setcap \u0026#39;cap_net_bind_service=+ep\u0026#39; /usr/bin/caddy chown root:root /usr/bin/caddy chown -R root:root /etc/caddy chown -R root:www-data /etc/ssl/caddy chown root:www-data /var/log/caddy.log chown root:root /etc/systemd/system/caddy.service chmod 755 /usr/bin/caddy chmod 770 /etc/ssl/caddy chmod 770 /var/log/caddy.log chmod 644 /etc/systemd/system/caddy.service chown root:root /usr/local/etc/v2ray/v2ray.cer chown root:root /usr/local/etc/v2ray/v2ray.key chmod 644 /usr/local/etc/v2ray/v2ray.cer chmod 644 /usr/local/etc/v2ray/v2ray.key chown root:root /usr/local/etc/ssl/ecc/password.cer chown root:root /usr/local/etc/ssl/ecc/password.key chmod 644 /usr/local/etc/ssl/ecc/password.cer chmod 644 /usr/local/etc/ssl/ecc/password.key chown root:root /usr/local/etc/v2ray/txt.cer chown root:root /usr/local/etc/v2ray/txt.key chmod 644 /usr/local/etc/v2ray/txt.cer chmod 644 /usr/local/etc/v2ray/txt.key mkdir /var/www/public vim /var/www/public/index.html 常用命令表 bash \u0026lt;(curl -L -s check.unlock.media) acme: export CF_Key=\u0026#34;\u0026#34; export CF_Email=\u0026#34;\u0026#34; acme.sh --register-account -m example@gmail.com acme.sh --issue --dns dns_cf -d example.com --keylength ec-256 --debug 2 acme.sh --installcert -d example.com --ecc --key-file /usr/local/etc/ssl/example.key --fullchain-file /usr/local/etc/ssl/example.cer --force caddy: vim /etc/caddy/caddyfile.json systemctl daemon-reload caddy list-modules\t#查看caddy安装了哪些插件 journalctl --boot -u caddy.service\t#查看caddy详细运行日志 systemctl restart caddy \u0026amp;\u0026amp; systemctl status caddy\t#重启caddy并查看运行状态 v2ray： vim /usr/local/etc/v2ray/config.json systemctl daemon-reload systemctl stop v2ray systemctl restart v2ray \u0026amp;\u0026amp; systemctl status v2ray journalctl -u v2ray bash install-release.sh\t#更新v2ray版本 ufw: utf allow 80\t#防火墙开启80端口允许外界访问， 需要提前apt安装ufw ufw delete 80\t#关闭80端口 ufw status numbered\t#查看开启了哪些端口 docker： docker run -d --name vaultwarden \\ -e SIGNUPS_ALLOWED=true \\ -e WEBSOCKET_ENABLED=true \\ -e LOG_FILE=/data/bitwarden.log \\ -p @1{vault_port}:80 \\ -p @2{vault_port}:3012 \\ -v /vw-data/:/data/ \\ vaultwarden/server:1.27.0 xcaddy： xcaddy build v2.6.0 \\ --with github.com/WingLim/caddy-webhook \\ --with github.com/mholt/caddy-webdav \\ --with github.com/vrongmeal/caddygit inotify: nohup bash inotify.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 其他 搭建所用的所有配置文件如上所示, 接下来写写我在搭建中遇到的一些问题\n原本v2ray使用的是h2+tls协议, 但是似乎没法在caddy v2中正常运行, 然后看到好多教程都是用ws, 我就也改成了ws, 然后就顺利运行了 bitwarden之前的官方docker镜像是可以直接监听443端口传来的https请求. 但新版的Vaultwarden要配备https的话好像很麻烦, 并且申请的证书是在docker容器里的, caddy无法访问. 但实际上可以先给域名申请证书, https的请求发给caddy之后再让caddy转发到aultwarden的docker容器映射的80端口那里. 这样实际访问的时候依然是https加密访问. 不用担心安全问题 参考资料 在搭建这篇博客时参考了以下链接, 感谢各位大佬的分享\ncaddy: Caddy2 简明教程\n利用Caddy实现Hugo个人博客的自动化部署\nvrongmeal/caddygit\nWingLim/caddy-webhook\nWingLim/caddy-webhook\nLadderOperator/docker-caddy2-hugo-alidns\ncaddy community\nvaultwarden: dani-garcia/vaultwarden\nhugo: hugo-command-usage\nHugo 目录组织\nHugo 静态博客食用指南\nStack主题\n","date":"March 16, 2023","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/daily/2023.03.16/","series":[],"smallImg":"","tags":[{"title":"caddy","url":"/tags/caddy/"},{"title":"webhook","url":"/tags/webhook/"},{"title":"hugo","url":"/tags/hugo/"},{"title":"v2ray","url":"/tags/v2ray/"},{"title":"bitwarden","url":"/tags/bitwarden/"}],"timestamp":1678903365,"title":"搭建自己的第一篇博客"},{"authors":[],"categories":[{"title":"Installation","url":"/categories/installation/"}],"content":"This guide show you how to install on Arch Linux.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/linux/archlinux/","series":[{"title":"Guide","url":"/series/guide/"}],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Arch Linux","url":"/tags/arch-linux/"}],"timestamp":1662475343,"title":"Install on Arch Linux"},{"authors":[],"categories":[{"title":"Installation","url":"/zh-hans/categories/installation/"}],"content":"This guide show you how to install on Arch Linux.\n","date":"September 6, 2022","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/zh-hans/docs/installation/linux/archlinux/","series":[{"title":"Guide","url":"/zh-hans/series/guide/"}],"smallImg":"","tags":[{"title":"Linux","url":"/zh-hans/tags/linux/"},{"title":"Arch Linux","url":"/zh-hans/tags/arch-linux/"}],"timestamp":1662475343,"title":"Install on Arch Linux"},{"authors":[],"categories":[],"content":"A fast, responsive and feature-rich Hugo theme for blog and documentations site.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/introduction/","series":[{"title":"Guide","url":"/series/guide/"}],"smallImg":"","tags":[],"timestamp":1662475343,"title":"Introduction"},{"authors":[],"categories":[{"title":"Installation","url":"/categories/installation/"}],"content":"This guide show you how to install on Ubuntu.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/linux/ubuntu/","series":[{"title":"Guide","url":"/series/guide/"}],"smallImg":"","tags":[{"title":"Linux","url":"/tags/linux/"},{"title":"Ubuntu","url":"/tags/ubuntu/"}],"timestamp":1662475343,"title":"Install on Ubuntu"},{"authors":[],"categories":[{"title":"Installation","url":"/zh-hans/categories/installation/"}],"content":"This guide show you how to install on Ubuntu.\n","date":"September 6, 2022","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/zh-hans/docs/installation/linux/ubuntu/","series":[{"title":"Guide","url":"/zh-hans/series/guide/"}],"smallImg":"","tags":[{"title":"Linux","url":"/zh-hans/tags/linux/"},{"title":"Ubuntu","url":"/zh-hans/tags/ubuntu/"}],"timestamp":1662475343,"title":"Install on Ubuntu"},{"authors":[],"categories":[{"title":"Installation","url":"/categories/installation/"}],"content":"This guide show you how to install on Windows.\n","date":"September 6, 2022","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/docs/installation/windows/","series":[{"title":"Guide","url":"/series/guide/"}],"smallImg":"","tags":[{"title":"Windows","url":"/tags/windows/"}],"timestamp":1662475343,"title":"Install on Windows"},{"authors":[],"categories":[{"title":"Installation","url":"/zh-hans/categories/installation/"}],"content":"This guide show you how to install on Windows.\n","date":"September 6, 2022","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/zh-hans/docs/installation/windows/","series":[{"title":"Guide","url":"/zh-hans/series/guide/"}],"smallImg":"","tags":[{"title":"Windows","url":"/zh-hans/tags/windows/"}],"timestamp":1662475343,"title":"Install on Windows"},{"authors":[],"categories":[],"content":"2e2558f1647e5c1759547b358a35eff8\n","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/baidu_verify_codeva-pvueso0gj0/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":""},{"authors":[],"categories":[],"content":"tioe.github.io ","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/readme/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":""},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/about/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"About"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Contact Us"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"en","langName":"English","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/zh-hans/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"authors":[],"categories":[],"content":"","date":"January 1, 1","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/zh-hans/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"联系我们"}]
